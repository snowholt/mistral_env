{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b0ba304",
   "metadata": {},
   "source": [
    "# API vs Direct Chat Manual Inspection Notebook\n",
    "This notebook provides two helper functions:\n",
    "1. **api_chat** - calls the running BeautyAI REST API.\n",
    "2. **direct_chat** - invokes the internal ChatService directly.\n",
    "\n",
    "You can: \n",
    "- Ask the *same* question with identical parameters.\n",
    "- Inspect whether responses match.\n",
    "- Compare output structure / format.\n",
    "- Check whether the same ChatService instance ID persists (printed).\n",
    "\n",
    "No regex cleaning, no auto comparison; just raw outputs for your manual review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e4da6e31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Functions ready: api_chat, direct_chat\n"
     ]
    }
   ],
   "source": [
    "import sys, json, logging, requests\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict\n",
    "\n",
    "# Silence noisy loggers\n",
    "logging.basicConfig(level=logging.CRITICAL)\n",
    "for noisy in [\"uvicorn\", \"uvicorn.error\", \"uvicorn.access\", \"httpx\", \"transformers\", \"torch\"]:\n",
    "    logging.getLogger(noisy).setLevel(logging.CRITICAL)\n",
    "\n",
    "# Add backend source\n",
    "backend_src = Path('..').resolve().parent / 'src'\n",
    "sys.path.insert(0, str(backend_src))\n",
    "\n",
    "from beautyai_inference.services.inference.chat_service import ChatService  # type: ignore\n",
    "from beautyai_inference.config.config_manager import AppConfig  # type: ignore\n",
    "from beautyai_inference.services.model.registry_service import ModelRegistryService  # type: ignore\n",
    "\n",
    "API_BASE_URL = 'http://127.0.0.1:8000'\n",
    "_CHAT_SERVICE = ChatService()\n",
    "_APP_CONFIG = AppConfig()\n",
    "_MODEL_REGISTRY_SERVICE = ModelRegistryService()\n",
    "\n",
    "def api_chat(model_name: str, message: str, **params) -> Dict[str, Any]:\n",
    "    payload = {'model_name': model_name, 'message': message} | params\n",
    "    r = requests.post(f'{API_BASE_URL}/inference/chat', json=payload, timeout=60)\n",
    "    try: return r.json()\n",
    "    except Exception: return {'error': True, 'status_code': r.status_code, 'text': r.text} \n",
    "\n",
    "def direct_chat(model_name: str, message: str, **params) -> Dict[str, Any]:\n",
    "    if not getattr(_APP_CONFIG, 'model_registry', None): _APP_CONFIG.load_model_registry()\n",
    "    model_cfg = _MODEL_REGISTRY_SERVICE.get_model(_APP_CONFIG, model_name)\n",
    "    if model_cfg is None: return {'error': f'model {model_name} not in registry'}\n",
    "    gen_cfg = params.copy()\n",
    "    for k in ['disable_content_filter','enable_thinking']: gen_cfg.pop(k, None)\n",
    "    response, detected_language, _, session_id = _CHAT_SERVICE.chat(\n",
    "        message=message, model_name=model_name, model_config=model_cfg,\n",
    "        generation_config=gen_cfg, conversation_history=[], response_language='auto',\n",
    "        session_id=None, disable_content_filter=params.get('disable_content_filter', False)\n",
    "    )\n",
    "    return {\n",
    "        'final_content': response,\n",
    "        'detected_language': detected_language,\n",
    "        'session_id': session_id,\n",
    "        'chat_service_id': id(_CHAT_SERVICE),\n",
    "        'model_name': model_name,\n",
    "    }\n",
    "print('Functions ready: api_chat, direct_chat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2e52ac74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== API RESULT RAW JSON ===\n",
      "{\n",
      "  \"success\": true,\n",
      "  \"data\": null,\n",
      "  \"timestamp\": \"2025-08-22T02:24:52.188558Z\",\n",
      "  \"execution_time_ms\": 127.55966186523438,\n",
      "  \"response\": \"‚Ä¶\",\n",
      "  \"session_id\": \"default\",\n",
      "  \"model_name\": \"qwen3-unsloth-q4ks\",\n",
      "  \"generation_stats\": {\n",
      "    \"model_info\": {},\n",
      "    \"generation_config_used\": {\n",
      "      \"temperature\": 0.0,\n",
      "      \"top_p\": 0.95,\n",
      "      \"top_k\": 20,\n",
      "      \"repetition_penalty\": 1.05,\n",
      "      \"max_new_tokens\": 200,\n",
      "      \"do_sample\": false,\n",
      "      \"enable_thinking\": false\n",
      "    },\n",
      "    \"content_filter_config\": {\n",
      "      \"strictness_level\": \"disabled\"\n",
      "    },\n",
      "    \"performance\": {\n",
      "      \"total_time_ms\": 127.55966186523438,\n",
      "      \"generation_time_ms\": 125.53858757019043,\n",
      "      \"tokens_generated\": 1,\n",
      "      \"tokens_per_second\": 7.96567827753268,\n",
      "      \"thinking_tokens\": 0\n",
      "    }\n",
      "  },\n",
      "  \"effective_config\": {\n",
      "    \"temperature\": 0.0,\n",
      "    \"top_p\": 0.95,\n",
      "    \"top_k\": 20,\n",
      "    \"repetition_penalty\": 1.05,\n",
      "    \"max_new_tokens\": 200,\n",
      "    \"do_sample\": false,\n",
      "    \"enable_thinking\": false\n",
      "  },\n",
      "  \"preset_used\": null,\n",
      "  \"thinking_enabled\": false,\n",
      "  \"content_filter_applied\": false,\n",
      "  \"content_filter_strictness\": \"disabled\",\n",
      "  \"content_filter_bypassed\": true,\n",
      "  \"tokens_generated\": 1,\n",
      "  \"generation_time_ms\": 125.53858757019043,\n",
      "  \"tokens_per_second\": 7.97,\n",
      "  \"thinking_content\": null,\n",
      "  \"final_content\": \"‚Ä¶\",\n",
      "  \"error\": null\n",
      "}\n",
      "=== DIRECT RESULT RAW JSON ===\n",
      "{\n",
      "  \"error\": \"model qwen3-unsloth-q4ks not in registry\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Example usage ‚Äî adjust model, message, and params as needed.\n",
    "model_name = 'qwen3-unsloth-q4ks'  # change if needed\n",
    "message = '/no_think What is botox?'\n",
    "params = {\n",
    "    'temperature': 0.0,\n",
    "    'top_p': 0.95,\n",
    "    'max_new_tokens': 200,\n",
    "    'do_sample': False,\n",
    "    'disable_content_filter': True,\n",
    "    'enable_thinking': False,\n",
    "} \n",
    "\n",
    "api_result = api_chat(model_name, message, **params)\n",
    "direct_result = direct_chat(model_name, message, **params)\n",
    "print('=== API RESULT RAW JSON ===')\n",
    "print(json.dumps(api_result, ensure_ascii=False, indent=2))\n",
    "print('=== DIRECT RESULT RAW JSON ===')\n",
    "print(json.dumps(direct_result, ensure_ascii=False, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ab907128",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing API with Arabic question...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status Code: 200\n",
      "Response: <think>\n",
      "Okay, the user is asking about the main benefit of Intense Pulsed Light (IPL) treatment. Let me recall what IPL is used for. It's a non-invasive procedure that uses broad-spectrum light to target various skin issues.\n",
      "First, I should mention the primary uses: things like sun damage, age spots, freckles, and uneven skin tone. Also, it can help with vascular issues like spider veins and rosacea. Maybe also mention hair removal as another application.\n",
      "But the user specifically asked for the \n",
      "Language: No language field\n"
     ]
    }
   ],
   "source": [
    "# Simple test to debug the model response generation\n",
    "import requests\n",
    "\n",
    "# Test just the API with a simple question\n",
    "test_payload = {\n",
    "    \"model_name\": \"qwen3-unsloth-q4ks\",\n",
    "    \"message\": \"ŸÖÿß ŸáŸä ÿßŸÑŸÅÿßÿ¶ÿØÿ© ÿßŸÑÿ±ÿ¶Ÿäÿ≥Ÿäÿ© ŸÑÿπŸÑÿßÿ¨ ÿßŸÑÿ∂Ÿàÿ° ÿßŸÑŸÜÿ®ÿØŸä ÿßŸÑŸÖŸÉÿ´ŸÅÿü\",\n",
    "    \"temperature\": 0.7,\n",
    "    \"max_new_tokens\": 150,\n",
    "    \"disable_content_filter\": True,\n",
    "}\n",
    "\n",
    "print(\"Testing API with Arabic question...\")\n",
    "response = requests.post('http://127.0.0.1:8000/inference/chat', json=test_payload, timeout=60)\n",
    "print(f\"Status Code: {response.status_code}\")\n",
    "\n",
    "if response.status_code == 200:\n",
    "    result = response.json()\n",
    "    print(f\"Response: {result.get('response', 'No response field')[:500]}\")\n",
    "    print(f\"Language: {result.get('language', 'No language field')}\")\n",
    "else:\n",
    "    print(f\"Error: {response.text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "acc52c40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing with /no_think instruction...\n",
      "Status Code: 200\n",
      "Response: <think>\n",
      "Okay, the user is asking about the main benefit of Intense Pulsed Light (IPL) treatment. Let me recall what IPL is used for. It's a non-invasive procedure that uses broad-spectrum light to target various skin issues.\n",
      "First, I should mention the primary uses: things like sun damage, pigmentation, and vascular lesions. The main benefit here is improving skin texture and appearance by reducing these issues. Also, IPL can help with hair removal, but the user might be more interested in skin rejuvenation.\n",
      "I need to make sure the answer is clear and concise, in Arabic only. Avoid medical jargon so it's easy for patients to understand. Highlight the key points: treating pigmentation, sunspots\n",
      "Language: No language field\n",
      "Response Length: 702\n"
     ]
    }
   ],
   "source": [
    "# Test the /no_think instruction specifically\n",
    "test_payload_no_think = {\n",
    "    \"model_name\": \"qwen3-unsloth-q4ks\",\n",
    "    \"message\": \"ŸÖÿß ŸáŸä ÿßŸÑŸÅÿßÿ¶ÿØÿ© ÿßŸÑÿ±ÿ¶Ÿäÿ≥Ÿäÿ© ŸÑÿπŸÑÿßÿ¨ ÿßŸÑÿ∂Ÿàÿ° ÿßŸÑŸÜÿ®ÿØŸä ÿßŸÑŸÖŸÉÿ´ŸÅÿü /no_think\",\n",
    "    \"temperature\": 0.7,\n",
    "    \"max_new_tokens\": 150,\n",
    "    \"disable_content_filter\": True,\n",
    "}\n",
    "\n",
    "print(\"Testing with /no_think instruction...\")\n",
    "response = requests.post('http://127.0.0.1:8000/inference/chat', json=test_payload_no_think, timeout=60)\n",
    "print(f\"Status Code: {response.status_code}\")\n",
    "\n",
    "if response.status_code == 200:\n",
    "    result = response.json()\n",
    "    print(f\"Response: {result.get('response', 'No response field')}\")\n",
    "    print(f\"Language: {result.get('language', 'No language field')}\")\n",
    "    print(f\"Response Length: {len(result.get('response', ''))}\")\n",
    "else:\n",
    "    print(f\"Error: {response.text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f7ddf59a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing with enable_thinking=False...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status Code: 200\n",
      "Response: ÿßŸÑÿπŸÑÿßÿ¨ ÿ®ÿßŸÑÿ∂Ÿàÿ° ÿßŸÑŸÜÿ®ÿØŸä ÿßŸÑŸÖŸÉÿ´ŸÅ (IPL) ŸäŸèÿ≥ÿ™ÿÆÿØŸÖ ÿ®ÿ¥ŸÉŸÑ ÿ±ÿ¶Ÿäÿ≥Ÿä ŸÑÿ™ÿ≠ÿ≥ŸäŸÜ ŸÖÿ∏Ÿáÿ± ÿßŸÑÿ®ÿ¥ÿ±ÿ© ŸÖŸÜ ÿÆŸÑÿßŸÑ ÿ™ŸÇŸÑŸäŸÑ ÿßŸÑÿ®ŸÇÿπ ÿßŸÑÿØÿßŸÉŸÜÿ©ÿå ÿßŸÑÿ™ÿµÿ®ÿ∫ÿßÿ™ÿå ŸàÿßŸÑÿ™Ÿáÿßÿ®ÿßÿ™ ÿßŸÑÿ®ÿ¥ÿ±ÿ©. ŸÉŸÖÿß Ÿäÿ≥ÿßÿπÿØ ŸÅŸä ÿ™ŸÇŸÑŸäŸÑ ÿ∏ŸáŸàÿ± ÿßŸÑÿ¥ÿπÿ± ÿßŸÑÿ≤ÿßÿ¶ÿØ Ÿàÿ™ÿ≠ÿ≥ŸäŸÜ ŸÜÿ∂ÿßÿ±ÿ© ÿßŸÑÿ®ÿ¥ÿ±ÿ©.\n",
      "Language: No language field\n",
      "Response Length: 186\n",
      "Thinking Enabled: False\n"
     ]
    }
   ],
   "source": [
    "# Test the fixed API with enable_thinking=False explicitly\n",
    "test_payload_fixed = {\n",
    "    \"model_name\": \"qwen3-unsloth-q4ks\",\n",
    "    \"message\": \"ŸÖÿß ŸáŸä ÿßŸÑŸÅÿßÿ¶ÿØÿ© ÿßŸÑÿ±ÿ¶Ÿäÿ≥Ÿäÿ© ŸÑÿπŸÑÿßÿ¨ ÿßŸÑÿ∂Ÿàÿ° ÿßŸÑŸÜÿ®ÿØŸä ÿßŸÑŸÖŸÉÿ´ŸÅÿü\",\n",
    "    \"temperature\": 0.3,\n",
    "    \"top_p\": 0.95,\n",
    "    \"max_new_tokens\": 150,\n",
    "    \"disable_content_filter\": True,\n",
    "    \"enable_thinking\": False,  # Explicitly disable thinking mode\n",
    "}\n",
    "\n",
    "print(\"Testing with enable_thinking=False...\")\n",
    "response = requests.post('http://127.0.0.1:8000/inference/chat', json=test_payload_fixed, timeout=60)\n",
    "print(f\"Status Code: {response.status_code}\")\n",
    "\n",
    "if response.status_code == 200:\n",
    "    result = response.json()\n",
    "    print(f\"Response: {result.get('response', 'No response field')}\")\n",
    "    print(f\"Language: {result.get('language', 'No language field')}\")\n",
    "    print(f\"Response Length: {len(result.get('response', ''))}\")\n",
    "    print(f\"Thinking Enabled: {result.get('thinking_enabled', 'N/A')}\")\n",
    "else:\n",
    "    print(f\"Error: {response.text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c4948b7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking recent API logs for enable_thinking debug messages...\n"
     ]
    }
   ],
   "source": [
    "# Check recent API logs for debug messages about enable_thinking\n",
    "import subprocess\n",
    "\n",
    "print(\"Checking recent API logs for enable_thinking debug messages...\")\n",
    "try:\n",
    "    result = subprocess.run([\n",
    "        \"sudo\", \"journalctl\", \"-u\", \"beautyai-api.service\", \n",
    "        \"--since\", \"2 minutes ago\", \"--no-pager\"\n",
    "    ], capture_output=True, text=True, timeout=10)\n",
    "    \n",
    "    lines = result.stdout.split('\\n')\n",
    "    relevant_lines = [line for line in lines if any(keyword in line.lower() for keyword in [\n",
    "        'enable_thinking', 'chat template', 'fallback', 'debug'\n",
    "    ])]\n",
    "    \n",
    "    for line in relevant_lines[-10:]:  # Show last 10 relevant lines\n",
    "        print(line)\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error checking logs: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ad4c072c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config -> host=127.0.0.1:8000 secure=False language=ar file=/home/lumi/beautyai/voice_tests/input_test_questions/pcm/q10.pcm\n",
      "‚úÖ Streaming voice status: 200\n",
      "   Enabled: True, Active sessions: 1\n",
      "\n",
      "Connecting to ws://127.0.0.1:8000/api/v1/ws/streaming-voice?language=ar\n",
      "Audio: 269 frames, 171870 bytes\n",
      "üîó WebSocket connected successfully!\n",
      "üì§ [sender] Starting to send audio frames...\n",
      "   Sent frame 100/269\n",
      "   Sent frame 200/269\n",
      "üîá [sender] Sending trailing silence...\n",
      "‚úÖ [sender] Finished sending all audio data\n",
      "üì§ Sender finished, waiting for processing...\n",
      "üì® [event #1] ready: \n",
      "üì® [event #2] decoder_started: \n",
      "üì® [event #3] ingest_mode: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì® [event #4] partial_transcript: ŸÖÿß ŸáŸä ÿßŸÑŸÅÿßÿ¶ÿØÿ© ÿßŸÑÿ±ÿ¶Ÿäÿ≥Ÿäÿ© ŸÑÿπŸÑÿßÿ¨ ÿßŸÑÿ∂Ÿàÿ° ÿßŸÑŸÜÿ®ÿØŸä ÿßŸÑŸÖŸÉÿ´ŸÅÿü\n",
      "üì® [event #5] metrics_snapshot: \n",
      "üì® [event #6] perf_cycle: \n",
      "üì® [event #7] perf_cycle: \n",
      "üì® [event #8] endpoint_event: \n",
      "üì® [event #9] perf_cycle: \n",
      "üì® [event #10] perf_cycle: \n",
      "üì® [event #11] endpoint_event: \n",
      "üì® [event #12] final_transcript: ŸÖÿß ŸáŸä ÿßŸÑŸÅÿßÿ¶ÿØÿ© ÿßŸÑÿ±ÿ¶Ÿäÿ≥Ÿäÿ© ŸÑÿπŸÑÿßÿ¨ ÿßŸÑÿ∂Ÿàÿ° ÿßŸÑŸÜÿ®ÿØŸä ÿßŸÑŸÖŸÉÿ´ŸÅÿü\n",
      "üé§ FINAL TRANSCRIPT: ŸÖÿß ŸáŸä ÿßŸÑŸÅÿßÿ¶ÿØÿ© ÿßŸÑÿ±ÿ¶Ÿäÿ≥Ÿäÿ© ŸÑÿπŸÑÿßÿ¨ ÿßŸÑÿ∂Ÿàÿ° ÿßŸÑŸÜÿ®ÿØŸä ÿßŸÑŸÖŸÉÿ´ŸÅÿü\n",
      "üì® [event #13] perf_cycle: \n",
      "üì® [event #14] assistant_pipeline_start: \n",
      "üì® [event #15] tts_start: \n",
      "üì® [event #16] endpoint_event: \n",
      "üì® [event #17] endpoint_event: \n",
      "üì® [event #18] heartbeat: \n",
      "üì® [event #19] endpoint_event: \n",
      "üì® [event #20] endpoint_event: \n",
      "üì® [event #21] endpoint_event: \n",
      "üì® [event #22] endpoint_event: \n",
      "üì® [event #23] assistant_response: ÿßŸÑŸÅÿßÿ¶ÿØÿ© ÿßŸÑÿ±ÿ¶Ÿäÿ≥Ÿäÿ© ŸÑÿπŸÑÿßÿ¨ ÿßŸÑÿ∂Ÿàÿ° ÿßŸÑŸÜÿ®ÿØŸä ÿßŸÑŸÖŸÉÿ´ŸÅ (IPL) ŸáŸä ÿ™ŸÇŸÑŸäŸÑ ÿßŸÑÿ®ŸÇÿπ ÿßŸÑÿØÿßŸÉŸÜÿ© ŸàÿßŸÑÿ™ÿµÿ®ÿ∫ÿßÿ™ ÿπŸÑŸâ ÿßŸÑÿ®ÿ¥ÿ±ÿ©ÿå Ÿàÿ™ÿ≠ÿ≥ŸäŸÜ\n",
      "üì® [event #24] endpoint_event: \n",
      "üì® [event #25] heartbeat: \n",
      "üì® [event #26] endpoint_event: \n",
      "üì® [event #27] endpoint_event: \n",
      "üì® [event #28] endpoint_event: \n",
      "üì® [event #29] endpoint_event: \n",
      "üì® [event #30] endpoint_event: \n",
      "üì® [event #31] endpoint_event: \n",
      "üì® [event #32] heartbeat: \n",
      "üì® [event #33] endpoint_event: \n",
      "üì® [event #34] tts_audio: ÿßŸÑŸÅÿßÿ¶ÿØÿ© ÿßŸÑÿ±ÿ¶Ÿäÿ≥Ÿäÿ© ŸÑÿπŸÑÿßÿ¨ ÿßŸÑÿ∂Ÿàÿ° ÿßŸÑŸÜÿ®ÿØŸä ÿßŸÑŸÖŸÉÿ´ŸÅ (IPL) ŸáŸä ÿ™ŸÇŸÑŸäŸÑ ÿßŸÑÿ®ŸÇÿπ ÿßŸÑÿØÿßŸÉŸÜÿ© ŸàÿßŸÑÿ™ÿµÿ®ÿ∫ÿßÿ™ ÿπŸÑŸâ ÿßŸÑÿ®ÿ¥ÿ±ÿ©ÿå Ÿàÿ™ÿ≠ÿ≥ŸäŸÜ\n",
      "üì® [event #35] tts_complete: \n",
      "üîä TTS COMPLETE\n",
      "‚úÖ Both transcript and TTS complete!\n",
      "‚úÖ Stream completed successfully!\n",
      "\n",
      "=== STREAMING DEBUG SUMMARY ===\n",
      "{\n",
      "  \"endpoint\": \"ws://127.0.0.1:8000\",\n",
      "  \"file\": \"/home/lumi/beautyai/voice_tests/input_test_questions/pcm/q10.pcm\",\n",
      "  \"bytes\": 171870,\n",
      "  \"frames\": 269,\n",
      "  \"language\": \"ar\",\n",
      "  \"final_transcript\": \"ŸÖÿß ŸáŸä ÿßŸÑŸÅÿßÿ¶ÿØÿ© ÿßŸÑÿ±ÿ¶Ÿäÿ≥Ÿäÿ© ŸÑÿπŸÑÿßÿ¨ ÿßŸÑÿ∂Ÿàÿ° ÿßŸÑŸÜÿ®ÿØŸä ÿßŸÑŸÖŸÉÿ´ŸÅÿü\",\n",
      "  \"tts_complete\": true,\n",
      "  \"events_total\": 35,\n",
      "  \"first_event_latency_ms\": 12,\n",
      "  \"duration_s\": 6.452\n",
      "}\n",
      "\n",
      "üèÅ Test completed. Success: True\n"
     ]
    }
   ],
   "source": [
    "# Raw Streaming Debug (no UI) - Test local streaming endpoint \n",
    "import asyncio, math, json, time, contextlib, os, ssl\n",
    "from pathlib import Path\n",
    "\n",
    "try:\n",
    "    import websockets  # type: ignore\n",
    "except ImportError:\n",
    "    raise RuntimeError(\"Install websockets: pip install websockets\")\n",
    "\n",
    "# Local server configuration (confirmed working)\n",
    "HOST = '127.0.0.1:8000'\n",
    "USE_SECURE = False  # Local server uses HTTP/WS\n",
    "LANGUAGE = 'ar'  # 'ar' | 'en' | 'auto'\n",
    "PCM_PATH = Path('/home/lumi/beautyai/voice_tests/input_test_questions/pcm/q10.pcm')\n",
    "FRAME_MS = 20\n",
    "FAST = True  # if False, will pace real-time\n",
    "TAIL_SILENCE_MS = 800\n",
    "AUTO_CLOSE_AFTER_FINAL_S = 15.0  # Increased timeout\n",
    "\n",
    "print(f\"Config -> host={HOST} secure={USE_SECURE} language={LANGUAGE} file={PCM_PATH}\")\n",
    "assert PCM_PATH.exists(), f\"PCM file not found: {PCM_PATH}\"\n",
    "\n",
    "# First, verify streaming endpoint is available\n",
    "import requests\n",
    "try:\n",
    "    stream_status = requests.get(f\"http://{HOST}/api/v1/ws/streaming-voice/status\", timeout=5)\n",
    "    print(f\"‚úÖ Streaming voice status: {stream_status.status_code}\")\n",
    "    if stream_status.status_code == 200:\n",
    "        status_data = stream_status.json()\n",
    "        print(f\"   Enabled: {status_data.get('enabled')}, Active sessions: {status_data.get('active_sessions')}\")\n",
    "    else:\n",
    "        print(f\"‚ùå Stream status error: {stream_status.text}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Local server connection failed: {e}\")\n",
    "\n",
    "async def stream_debug():\n",
    "    pcm_bytes = PCM_PATH.read_bytes()\n",
    "    samples_per_frame = int(16000 * FRAME_MS / 1000)\n",
    "    frame_bytes = samples_per_frame * 2\n",
    "    total_frames = math.ceil(len(pcm_bytes)/frame_bytes)\n",
    "    scheme = 'wss' if USE_SECURE else 'ws'\n",
    "    url = f\"{scheme}://{HOST}/api/v1/ws/streaming-voice?language={LANGUAGE}\"\n",
    "    print(f\"\\nConnecting to {url}\")\n",
    "    print(f\"Audio: {total_frames} frames, {len(pcm_bytes)} bytes\")\n",
    "\n",
    "    events = []\n",
    "    final_transcript = None\n",
    "    tts_complete = False\n",
    "    first_event_time = None\n",
    "    start = time.time()\n",
    "\n",
    "    async def sender(ws):\n",
    "        print(\"üì§ [sender] Starting to send audio frames...\")\n",
    "        cursor = 0\n",
    "        frame_index = 0\n",
    "        \n",
    "        while cursor < len(pcm_bytes):\n",
    "            chunk = pcm_bytes[cursor: cursor+frame_bytes]\n",
    "            cursor += frame_bytes\n",
    "            frame_index += 1\n",
    "            \n",
    "            try:\n",
    "                await ws.send(chunk)\n",
    "                if frame_index % 100 == 0:  # Progress every 100 frames\n",
    "                    print(f\"   Sent frame {frame_index}/{total_frames}\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå [sender] Send error at frame {frame_index}: {e}\")\n",
    "                return\n",
    "                \n",
    "            if not FAST:\n",
    "                await asyncio.sleep(FRAME_MS/1000)\n",
    "        \n",
    "        # Send trailing silence\n",
    "        print(\"üîá [sender] Sending trailing silence...\")\n",
    "        silence_frames = max(1, int(TAIL_SILENCE_MS/FRAME_MS))\n",
    "        silence_chunk = b\"\\x00\\x00\" * samples_per_frame\n",
    "        \n",
    "        for i in range(silence_frames):\n",
    "            try:\n",
    "                await ws.send(silence_chunk)\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå [sender] Trailing silence send error: {e}\")\n",
    "                break\n",
    "            if not FAST:\n",
    "                await asyncio.sleep(FRAME_MS/1000)\n",
    "                \n",
    "        print(\"‚úÖ [sender] Finished sending all audio data\")\n",
    "\n",
    "    async def receiver(ws, final_event):\n",
    "        nonlocal final_transcript, tts_complete, first_event_time\n",
    "        message_count = 0\n",
    "        \n",
    "        try:\n",
    "            while True:\n",
    "                try:\n",
    "                    msg = await asyncio.wait_for(ws.recv(), timeout=2.0)\n",
    "                    message_count += 1\n",
    "                except asyncio.TimeoutError:\n",
    "                    print(\"‚è∞ [receiver] No message received for 2 seconds, continuing...\")\n",
    "                    continue\n",
    "                except Exception as e:\n",
    "                    print(f\"üîö [receiver] Recv ended after {message_count} messages: {e}\")\n",
    "                    break\n",
    "                \n",
    "                now = time.time()\n",
    "                if first_event_time is None:\n",
    "                    first_event_time = now\n",
    "                    \n",
    "                try:\n",
    "                    data = json.loads(msg)\n",
    "                except json.JSONDecodeError:\n",
    "                    print(f\"üìÑ [receiver] Non-JSON message #{message_count}, length={len(msg)}\")\n",
    "                    continue\n",
    "                    \n",
    "                events.append(data)\n",
    "                etype = data.get('type')\n",
    "                content = data.get('text', data.get('content', ''))[:100]  # Truncate long content\n",
    "                print(f\"üì® [event #{message_count}] {etype}: {content}\")\n",
    "                \n",
    "                if etype == 'final_transcript' and final_transcript is None:\n",
    "                    final_transcript = data.get('text')\n",
    "                    print(f\"üé§ FINAL TRANSCRIPT: {final_transcript}\")\n",
    "                    \n",
    "                if etype == 'tts_complete':\n",
    "                    tts_complete = True\n",
    "                    print(\"üîä TTS COMPLETE\")\n",
    "                    \n",
    "                if final_transcript and tts_complete:\n",
    "                    print(\"‚úÖ Both transcript and TTS complete!\")\n",
    "                    final_event.set()\n",
    "                    break\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå [receiver] Error: {e}\")\n",
    "\n",
    "    try:\n",
    "        async with websockets.connect(url, ping_interval=30) as ws:\n",
    "            print(\"üîó WebSocket connected successfully!\")\n",
    "            \n",
    "            final_event = asyncio.Event()\n",
    "            \n",
    "            # Start both sender and receiver\n",
    "            sender_task = asyncio.create_task(sender(ws))\n",
    "            receiver_task = asyncio.create_task(receiver(ws, final_event))\n",
    "            \n",
    "            # Wait for sender to complete\n",
    "            await sender_task\n",
    "            print(\"üì§ Sender finished, waiting for processing...\")\n",
    "            \n",
    "            # Wait for final event or timeout\n",
    "            try:\n",
    "                await asyncio.wait_for(final_event.wait(), timeout=AUTO_CLOSE_AFTER_FINAL_S)\n",
    "                print(\"‚úÖ Stream completed successfully!\")\n",
    "            except asyncio.TimeoutError:\n",
    "                print(f\"‚è∞ Timeout waiting for completion ({AUTO_CLOSE_AFTER_FINAL_S}s)\")\n",
    "            \n",
    "            # Cancel receiver task\n",
    "            receiver_task.cancel()\n",
    "            \n",
    "            try:\n",
    "                await receiver_task\n",
    "            except asyncio.CancelledError:\n",
    "                pass\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå WebSocket connection error: {e}\")\n",
    "\n",
    "    end = time.time()\n",
    "    print(\"\\n=== STREAMING DEBUG SUMMARY ===\")\n",
    "    summary = {\n",
    "        'endpoint': f\"{scheme}://{HOST}\",\n",
    "        'file': str(PCM_PATH),\n",
    "        'bytes': len(pcm_bytes),\n",
    "        'frames': total_frames,\n",
    "        'language': LANGUAGE,\n",
    "        'final_transcript': final_transcript,\n",
    "        'tts_complete': tts_complete,\n",
    "        'events_total': len(events),\n",
    "        'first_event_latency_ms': int((first_event_time-start)*1000) if first_event_time else None,\n",
    "        'duration_s': round(end-start, 3),\n",
    "    }\n",
    "    print(json.dumps(summary, ensure_ascii=False, indent=2))\n",
    "    \n",
    "    return summary\n",
    "\n",
    "# Run the stream debug\n",
    "result = await stream_debug()\n",
    "print(f\"\\nüèÅ Test completed. Success: {result['final_transcript'] is not None}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23a89d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test remote API availability and streaming voice status\n",
    "import requests\n",
    "import ssl\n",
    "import urllib3\n",
    "\n",
    "# Disable SSL warnings for self-signed certificates\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "\n",
    "def test_remote_api():\n",
    "    base_url = \"https://api.gmai.sa\"\n",
    "    \n",
    "    # Test basic API health\n",
    "    try:\n",
    "        response = requests.get(f\"{base_url}/health\", verify=False, timeout=10)\n",
    "        print(f\"API Health Status: {response.status_code}\")\n",
    "        if response.status_code == 200:\n",
    "            print(f\"API Health Response: {response.json()}\")\n",
    "    except Exception as e:\n",
    "        print(f\"API Health Check Failed: {e}\")\n",
    "    \n",
    "    # Test streaming voice status endpoint\n",
    "    try:\n",
    "        response = requests.get(f\"{base_url}/api/v1/ws/streaming-voice/status\", verify=False, timeout=10)\n",
    "        print(f\"Streaming Voice Status: {response.status_code}\")\n",
    "        if response.status_code == 200:\n",
    "            print(f\"Streaming Voice Response: {response.json()}\")\n",
    "        else:\n",
    "            print(f\"Error Response: {response.text}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Streaming Voice Status Check Failed: {e}\")\n",
    "        \n",
    "    # Test basic inference endpoint\n",
    "    try:\n",
    "        test_payload = {\n",
    "            \"model_name\": \"qwen3-unsloth-q4ks\",\n",
    "            \"message\": \"Hello test\",\n",
    "            \"disable_content_filter\": True,\n",
    "            \"max_new_tokens\": 10\n",
    "        }\n",
    "        response = requests.post(f\"{base_url}/inference/chat\", json=test_payload, verify=False, timeout=30)\n",
    "        print(f\"Inference Test Status: {response.status_code}\")\n",
    "        if response.status_code == 200:\n",
    "            result = response.json()\n",
    "            print(f\"Inference Test Success - Response length: {len(result.get('response', ''))}\")\n",
    "        else:\n",
    "            print(f\"Inference Error: {response.text[:200]}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Inference Test Failed: {e}\")\n",
    "\n",
    "test_remote_api()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e813442",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test remote WSS endpoint that user confirmed working in browser\n",
    "# This is just to verify the connection format is correct\n",
    "import asyncio\n",
    "import ssl\n",
    "import json\n",
    "\n",
    "try:\n",
    "    import websockets  # type: ignore\n",
    "except ImportError:\n",
    "    raise RuntimeError(\"Install websockets: pip install websockets\")\n",
    "\n",
    "async def test_remote_wss_connection():\n",
    "    \"\"\"Test the remote WSS endpoint format that user confirmed works in browser\"\"\"\n",
    "    url = \"wss://api.gmai.sa/api/v1/ws/streaming-voice?language=ar\"\n",
    "    print(f\"Testing remote WSS connection: {url}\")\n",
    "    \n",
    "    # Create SSL context for self-signed certificate\n",
    "    ssl_context = ssl.create_default_context()\n",
    "    ssl_context.check_hostname = False\n",
    "    ssl_context.verify_mode = ssl.CERT_NONE\n",
    "    \n",
    "    try:\n",
    "        # Try to connect to verify the endpoint format\n",
    "        async with websockets.connect(url, ssl=ssl_context, ping_interval=30) as ws:\n",
    "            print(\"‚úÖ Remote WSS connection successful!\")\n",
    "            print(\"üéß Connection headers confirm WebSocket upgrade worked\")\n",
    "            \n",
    "            # Send a small test to see if we get a response\n",
    "            await asyncio.sleep(1)  # Give server a moment\n",
    "            \n",
    "            # Check if any initial messages\n",
    "            try:\n",
    "                msg = await asyncio.wait_for(ws.recv(), timeout=3.0)\n",
    "                data = json.loads(msg)\n",
    "                print(f\"üì® Received initial message: {data}\")\n",
    "            except asyncio.TimeoutError:\n",
    "                print(\"‚è∞ No initial message (normal for streaming endpoint)\")\n",
    "            except Exception as e:\n",
    "                print(f\"üìÑ Initial message error: {e}\")\n",
    "                \n",
    "            print(\"üîö Closing test connection\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Remote WSS connection failed: {e}\")\n",
    "        print(\"   This might be due to network restrictions or server configuration\")\n",
    "\n",
    "# Test the remote endpoint format\n",
    "await test_remote_wss_connection()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
