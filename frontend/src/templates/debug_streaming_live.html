<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>BeautyAI Voice Streaming Debug Console</title>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/debug_streaming.css') }}">
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">
  <style>
    .language-mismatch-banner {
      background: linear-gradient(135deg, #ff9800, #f57c00);
      color: white;
      padding: 12px;
      margin-bottom: 20px;
      border-radius: 8px;
      box-shadow: 0 2px 10px rgba(255, 152, 0, 0.3);
      animation: slideDown 0.3s ease-out;
    }
    
    .banner-content {
      display: flex;
      align-items: center;
      gap: 15px;
    }
    
    .banner-content i {
      font-size: 18px;
    }
    
    .banner-actions {
      margin-left: auto;
      display: flex;
      gap: 8px;
    }
    
    .btn-sm {
      padding: 4px 12px;
      font-size: 12px;
      border-radius: 4px;
    }
    
    @keyframes slideDown {
      from { transform: translateY(-100%); opacity: 0; }
      to { transform: translateY(0); opacity: 1; }
    }
    
    /* Device Selection Styles */
    .form-grid {
      display: grid;
      grid-template-columns: 1fr 1fr;
      gap: 15px;
      margin-bottom: 20px;
    }
    
    .form-group {
      display: flex;
      flex-direction: column;
      gap: 5px;
    }
    
    .form-group label {
      font-weight: 600;
      color: #555;
      font-size: 0.9rem;
    }
    
    .form-group select {
      padding: 8px;
      border: 1px solid #ddd;
      border-radius: 4px;
      font-size: 0.9rem;
    }
    
    .checkbox-group {
      display: flex;
      align-items: center;
      gap: 8px;
    }
    
    .checkbox-group input[type="checkbox"] {
      transform: scale(1.2);
    }
    
    .checkbox-group label {
      margin: 0;
      font-weight: normal;
    }
    
    .form-group small {
      color: #666;
      font-size: 0.8rem;
      font-style: italic;
    }
    
    /* Echo Test Styles */
    .test-results {
      background: #f8f9fa;
      border: 1px solid #e9ecef;
      border-radius: 8px;
      padding: 15px;
      margin-top: 10px;
    }
    
    .metrics-grid {
      display: grid;
      grid-template-columns: repeat(auto-fit, minmax(150px, 1fr));
      gap: 15px;
    }
    
    .metric {
      text-align: center;
    }
    
    .metric-label {
      font-size: 0.8rem;
      color: #666;
      margin-bottom: 5px;
    }
    
    .metric-value {
      font-size: 1.1rem;
      font-weight: 600;
      color: #333;
    }
    
    #echo-detected.positive {
      color: #dc3545;
    }
    
    #echo-detected.negative {
      color: #28a745;
    }
    
    #vad-activity.active {
      color: #007bff;
    }
    
    .btn-sm {
      padding: 4px 8px;
      font-size: 0.8rem;
      margin-left: 8px;
    }
  </style>
</head>
<body>
  <div class="container">
    <header class="header">
      <h1><i class="fas fa-microphone-alt"></i> BeautyAI Voice Streaming Debug Console</h1>
      <p class="subtitle">Real-time WebSocket voice streaming with comprehensive debugging tools</p>
    </header>

    <!-- Control Panel -->
    <section class="control-panel">
      <div class="card">
        <h3><i class="fas fa-cogs"></i> Connection Settings</h3>
        <div class="form-grid">
          <div class="form-group">
            <label for="wsUrl">WebSocket URL:</label>
            <select id="wsUrlSelect">
              <option value="wss://api.gmai.sa/api/v1/ws/streaming-voice">Streaming Voice (Production)</option>
              <option value="wss://api.gmai.sa/ws/simple-voice">Simple Voice (Production)</option>
              <option value="ws://localhost:8000/api/v1/ws/streaming-voice">Streaming Voice (Local)</option>
              <option value="ws://localhost:8000/ws/simple-voice">Simple Voice (Local)</option>
            </select>
          </div>
          <div class="form-group">
            <label for="language">Language:</label>
            <select id="language">
              <option value="en">English</option>
              <option value="ar">Arabic</option>
              <option value="auto">Auto-detect</option>
            </select>
          </div>
          <div class="form-group">
            <label for="chunkMs">Chunk Size (ms):</label>
            <input id="chunkMs" type="number" value="200" min="100" max="500" step="50">
            <small>Lower = better latency, Higher = better efficiency</small>
          </div>
          <div class="form-group">
            <label for="captureMethod">Capture Method:</label>
            <select id="captureMethod">
              <option value="auto">Auto (AudioWorklet preferred)</option>
              <option value="worklet">AudioWorklet (Low latency)</option>
              <option value="script">ScriptProcessor (Fallback)</option>
            </select>
          </div>
        </div>
        
        <!-- Audio Device Settings -->
        <h4><i class="fas fa-volume-up"></i> Audio Device Settings</h4>
        <div class="form-grid">
          <div class="form-group">
            <label for="microphoneDevice">Microphone:</label>
            <select id="microphoneDevice">
              <option value="">Default</option>
            </select>
            <button id="btn-refresh-devices" class="btn btn-sm btn-secondary">
              <i class="fas fa-refresh"></i> Refresh
            </button>
          </div>
          <div class="form-group">
            <label for="speakerDevice">Speaker:</label>
            <select id="speakerDevice">
              <option value="">Default</option>
            </select>
          </div>
          <div class="form-group">
            <label>Echo Cancellation:</label>
            <div class="checkbox-group">
              <input type="checkbox" id="echoCancellation" checked>
              <label for="echoCancellation">Enable echo cancellation</label>
            </div>
            <small>Prevents TTS output from leaking into microphone</small>
          </div>
          <div class="form-group">
            <label>Duplex Mode:</label>
            <div class="checkbox-group">
              <input type="checkbox" id="duplexMode" checked>
              <label for="duplexMode">Enable full duplex streaming</label>
            </div>
            <small>Real-time bidirectional audio streaming</small>
          </div>
        </div>
        
        <!-- Echo Test Controls -->
        <h4><i class="fas fa-vial"></i> Echo Test</h4>
        <div class="button-group">
          <button id="btn-echo-test" class="btn btn-warning">
            <i class="fas fa-play"></i> Run Echo Test
          </button>
          <button id="btn-stop-echo-test" class="btn btn-secondary" disabled>
            <i class="fas fa-stop"></i> Stop Test
          </button>
        </div>
        <div id="echo-test-results" class="test-results" style="display: none;">
          <div class="metrics-grid">
            <div class="metric">
              <div class="metric-label">Echo Detected:</div>
              <div id="echo-detected" class="metric-value">No</div>
            </div>
            <div class="metric">
              <div class="metric-label">Correlation:</div>
              <div id="echo-correlation" class="metric-value">0.0</div>
            </div>
            <div class="metric">
              <div class="metric-label">VAD Activity:</div>
              <div id="vad-activity" class="metric-value">Inactive</div>
            </div>
          </div>
        </div>
        
        <!-- Connection Controls -->
        <h4><i class="fas fa-plug"></i> Connection</h4>
        <div class="button-group">
          <button id="btn-connect" class="btn btn-primary">
            <i class="fas fa-plug"></i> Connect
          </button>
          <button id="btn-disconnect" class="btn btn-danger" disabled>
            <i class="fas fa-times"></i> Disconnect
          </button>
          <button id="btn-reset-conversation" class="btn btn-warning" disabled>
            <i class="fas fa-refresh"></i> Reset Conversation
          </button>
          <button id="btn-clear" class="btn btn-secondary">
            <i class="fas fa-trash"></i> Clear All
          </button>
          <button id="btn-export" class="btn btn-info">
            <i class="fas fa-download"></i> Export Logs
          </button>
        </div>
      </div>
    </section>

    <!-- Language Mismatch Banner -->
    <div id="language-mismatch-banner" class="language-mismatch-banner" style="display: none;">
      <div class="banner-content">
        <i class="fas fa-language"></i>
        <span id="mismatch-message">Language mismatch detected</span>
        <div class="banner-actions">
          <button id="btn-switch-language" class="btn btn-sm btn-primary">Switch</button>
          <button id="btn-ignore-mismatch" class="btn btn-sm btn-secondary">Ignore</button>
          <button id="btn-close-banner" class="btn btn-sm btn-danger">Ã—</button>
        </div>
      </div>
    </div>

    <!-- Status Dashboard -->
    <section class="status-dashboard">
      <div class="status-grid">
        <div class="status-card">
          <i class="fas fa-wifi status-icon" id="connection-icon"></i>
          <div class="status-info">
            <h4>Connection</h4>
            <span id="connection-status" class="status-disconnected">Disconnected</span>
          </div>
        </div>
        
        <div class="status-card">
          <i class="fas fa-microphone status-icon" id="mic-icon"></i>
          <div class="status-info">
            <h4>Microphone</h4>
            <span id="mic-status" class="status-inactive">Inactive</span>
          </div>
        </div>
        
        <div class="status-card">
          <i class="fas fa-clock status-icon"></i>
          <div class="status-info">
            <h4>Latency</h4>
            <span id="latency-display">-- ms</span>
          </div>
        </div>
        
        <div class="status-card">
          <i class="fas fa-exchange-alt status-icon"></i>
          <div class="status-info">
            <h4>Data Rate</h4>
            <span id="data-rate">-- KB/s</span>
          </div>
        </div>
      </div>
      
      <!-- Audio Level Meter -->
      <div class="audio-meter-container">
        <label>Audio Level:</label>
        <div class="audio-meter">
          <div class="audio-level-bar" id="audio-level-bar"></div>
          <div class="audio-meter-marks">
            <span>-60</span><span>-40</span><span>-20</span><span>0 dB</span>
          </div>
        </div>
      </div>
    </section>

    <!-- Performance Monitoring Dashboard -->
    <section class="performance-section">
      <div class="card">
        <h3><i class="fas fa-tachometer-alt"></i> Performance Monitoring Dashboard</h3>
        <div class="dashboard-controls">
          <button id="btn-refresh-dashboard" class="btn btn-sm btn-secondary">
            <i class="fas fa-sync"></i> Refresh
          </button>
          <button id="btn-toggle-dashboard" class="btn btn-sm btn-info">
            <i class="fas fa-eye"></i> Toggle
          </button>
          <select id="dashboard-metrics" class="form-control" style="width: auto; display: inline-block; margin-left: 10px;">
            <option value="streaming,connection_pool,circuit_breaker">All Metrics</option>
            <option value="streaming">Streaming Only</option>
            <option value="connection_pool">Connection Pool</option>
            <option value="circuit_breaker">Circuit Breaker</option>
            <option value="system">System Resources</option>
          </select>
        </div>
        <div class="performance-dashboard">
          <iframe 
            id="performance-dashboard-frame"
            src="/api/v1/performance/dashboard?embed=true&metrics=streaming,connection_pool,circuit_breaker"
            width="100%" 
            height="500"
            frameborder="0"
            style="border-radius: 8px; background: white; margin-top: 10px;">
          </iframe>
        </div>
      </div>
    </section>

    <!-- Main Content -->
    <div class="main-content">
      <!-- Conversation Panel -->
      <section class="conversation-panel">
        <div class="card">
          <h3><i class="fas fa-comments"></i> Live Conversation</h3>
          <div id="conversation-container" class="conversation-container">
            <!-- Conversation messages will be added here dynamically -->
          </div>
          <div class="conversation-stats">
            <span>Turns: <span id="turn-count">0</span></span>
            <span>Avg Response Time: <span id="avg-response-time">-- ms</span></span>
          </div>
        </div>
      </section>

      <!-- Technical Logs -->
      <section class="logs-panel">
        <div class="card">
          <h3><i class="fas fa-terminal"></i> Technical Events</h3>
          <div class="tabs">
            <button class="tab-btn active" data-tab="events">Events</button>
            <button class="tab-btn" data-tab="metrics">Metrics</button>
            <button class="tab-btn" data-tab="transcription">Transcription</button>
            <button class="tab-btn" data-tab="assistant">Assistant</button>
          </div>
          
          <div id="events-tab" class="tab-content active">
            <div id="log" class="log"></div>
          </div>
          
          <div id="metrics-tab" class="tab-content">
            <div class="metrics-grid">
              <div class="metric-item">
                <label>Total Messages:</label>
                <span id="total-messages">0</span>
              </div>
              <div class="metric-item">
                <label>Audio Chunks Sent:</label>
                <span id="audio-chunks-sent">0</span>
              </div>
              <div class="metric-item">
                <label>Transcription Events:</label>
                <span id="transcription-events">0</span>
              </div>
              <div class="metric-item">
                <label>TTS Events:</label>
                <span id="tts-events">0</span>
              </div>
              <div class="metric-item">
                <label>Connection Uptime:</label>
                <span id="connection-uptime">--</span>
              </div>
              <div class="metric-item">
                <label>Last Activity:</label>
                <span id="last-activity">Never</span>
              </div>
            </div>
          </div>
          
          <div id="transcription-tab" class="tab-content">
            <textarea id="transcript" readonly placeholder="Transcribed text will appear here..."></textarea>
          </div>
          
          <div id="assistant-tab" class="tab-content">
            <textarea id="assistant" readonly placeholder="Assistant responses will appear here..."></textarea>
          </div>
        </div>
      </section>
    </div>
  </div>

  <script>
    // Global state
    let ws, audioCtx, source, processor, mediaStream, analyser;
    let currentPartial = '';
    let connectionStart = null;
    let currentAudioLevel = 0; // Track current audio level
    let audioLevelHistory = []; // Track audio level history for smarter thresholding
    let lastSignificantAudio = 0; // Timestamp of last significant audio
    let metrics = {
      totalMessages: 0,
      audioChunksSent: 0,
      transcriptionEvents: 0,
      ttsEvents: 0,
      responseTimes: [],
      lastActivity: null
    };
    let logEntries = [];
    let conversationTurns = [];
    let currentUtteranceStart = null;
    
    // Assistant response deduplication tracking
    let assistantResponses = new Map(); // utterance_index -> response_data
    let lastLanguageMismatch = null;

    // DOM elements
    const logEl = document.getElementById('log');
    const transcriptEl = document.getElementById('transcript');
    const assistantEl = document.getElementById('assistant');
    const wsUrlSelectEl = document.getElementById('wsUrlSelect');
    const languageEl = document.getElementById('language');
    const chunkMsEl = document.getElementById('chunkMs');
    const btnConnect = document.getElementById('btn-connect');
    const btnDisconnect = document.getElementById('btn-disconnect');
    const btnResetConversation = document.getElementById('btn-reset-conversation');
    const btnClear = document.getElementById('btn-clear');
    const btnExport = document.getElementById('btn-export');
    const conversationContainer = document.getElementById('conversation-container');
    const audioLevelBar = document.getElementById('audio-level-bar');

    // Initialize tabs
    document.querySelectorAll('.tab-btn').forEach(btn => {
      btn.addEventListener('click', () => {
        const tabName = btn.dataset.tab;
        document.querySelectorAll('.tab-btn').forEach(b => b.classList.remove('active'));
        document.querySelectorAll('.tab-content').forEach(t => t.classList.remove('active'));
        btn.classList.add('active');
        document.getElementById(`${tabName}-tab`).classList.add('active');
      });
    });

    function updateStatus(element, status, className) {
      element.textContent = status;
      element.className = `status-${className}`;
    }

    function updateConnectionStatus(status) {
      const statusEl = document.getElementById('connection-status');
      const iconEl = document.getElementById('connection-icon');
      
      switch(status) {
        case 'connected':
          updateStatus(statusEl, 'Connected', 'connected');
          iconEl.className = 'fas fa-wifi status-icon status-connected';
          break;
        case 'connecting':
          updateStatus(statusEl, 'Connecting...', 'connecting');
          iconEl.className = 'fas fa-spinner fa-spin status-icon status-connecting';
          break;
        case 'disconnected':
          updateStatus(statusEl, 'Disconnected', 'disconnected');
          iconEl.className = 'fas fa-wifi-slash status-icon status-disconnected';
          break;
      }
    }

    function updateMicStatus(status) {
      const statusEl = document.getElementById('mic-status');
      const iconEl = document.getElementById('mic-icon');
      
      switch(status) {
        case 'active':
          updateStatus(statusEl, 'Active', 'connected');
          iconEl.className = 'fas fa-microphone status-icon status-connected';
          break;
        case 'inactive':
          updateStatus(statusEl, 'Inactive', 'inactive');
          iconEl.className = 'fas fa-microphone-slash status-icon status-inactive';
          break;
      }
    }

    function showLanguageMismatchBanner(detectedLang, expectedLang) {
      const banner = document.getElementById('language-mismatch-banner');
      const message = document.getElementById('mismatch-message');
      
      message.textContent = `Detected ${detectedLang} speech, but expected ${expectedLang}. Switch language?`;
      banner.style.display = 'block';
      
      lastLanguageMismatch = { detected: detectedLang, expected: expectedLang };
    }

    function hideLanguageMismatchBanner() {
      document.getElementById('language-mismatch-banner').style.display = 'none';
    }

    function switchLanguage() {
      if (lastLanguageMismatch) {
        document.getElementById('language').value = lastLanguageMismatch.detected;
        log(`Language switched to: ${lastLanguageMismatch.detected}`, 'info');
        hideLanguageMismatchBanner();
        
        // Optionally reconnect with new language
        if (ws && ws.readyState === WebSocket.OPEN) {
          log('Reconnecting with new language...', 'info');
          disconnect();
          setTimeout(connect, 500);
        }
      }
    }

    function log(msg, type = 'info') {
      const timestamp = new Date().toISOString();
      const timeStr = timestamp.split('T')[1].replace('Z', '');
      const entry = { timestamp, message: msg, type };
      logEntries.push(entry);
      
      const logLine = `[${timeStr}] ${msg}\n`;
      logEl.textContent += logLine;
      logEl.scrollTop = logEl.scrollHeight;
      
      metrics.lastActivity = timestamp;
      updateMetrics();
    }

    function addConversationMessage(type, text, timestamp = new Date()) {
      const messageEl = document.createElement('div');
      messageEl.className = `conversation-message ${type}`;
      
      const emoji = type === 'user' ? 'ðŸ‘¤' : 'ðŸ¤–';
      const timeStr = timestamp.toLocaleTimeString();
      
      messageEl.innerHTML = `
        <div class="message-header">
          <span class="message-emoji">${emoji}</span>
          <span class="message-time">${timeStr}</span>
        </div>
        <div class="message-text">${text}</div>
      `;
      
      conversationContainer.appendChild(messageEl);
      conversationContainer.scrollTop = conversationContainer.scrollHeight;
      
      if (type === 'user') {
        currentUtteranceStart = Date.now();
      } else if (type === 'assistant' && currentUtteranceStart) {
        const responseTime = Date.now() - currentUtteranceStart;
        metrics.responseTimes.push(responseTime);
        updateResponseTimeMetrics();
      }
      
      // Update turn count
      if (type === 'assistant') {
        conversationTurns.push({ user: text, timestamp });
        document.getElementById('turn-count').textContent = conversationTurns.length;
      }
    }

    function updateResponseTimeMetrics() {
      if (metrics.responseTimes.length > 0) {
        const avg = metrics.responseTimes.reduce((a, b) => a + b, 0) / metrics.responseTimes.length;
        document.getElementById('avg-response-time').textContent = `${Math.round(avg)} ms`;
        
        const latest = metrics.responseTimes[metrics.responseTimes.length - 1];
        document.getElementById('latency-display').textContent = `${latest} ms`;
      }
    }

    function updateMetrics() {
      document.getElementById('total-messages').textContent = metrics.totalMessages;
      document.getElementById('audio-chunks-sent').textContent = metrics.audioChunksSent;
      document.getElementById('transcription-events').textContent = metrics.transcriptionEvents;
      document.getElementById('tts-events').textContent = metrics.ttsEvents;
      document.getElementById('last-activity').textContent = 
        metrics.lastActivity ? new Date(metrics.lastActivity).toLocaleTimeString() : 'Never';
      
      if (connectionStart) {
        const uptime = Math.floor((Date.now() - connectionStart) / 1000);
        const minutes = Math.floor(uptime / 60);
        const seconds = uptime % 60;
        document.getElementById('connection-uptime').textContent = `${minutes}:${seconds.toString().padStart(2, '0')}`;
      }
    }

    function updateAudioLevel(level) {
      // Convert to dB scale (-60 to 0)
      const db = 20 * Math.log10(level + 0.00001); // Add small value to avoid log(0)
      const normalizedLevel = Math.max(0, (db + 60) / 60); // Normalize to 0-1
      
      audioLevelBar.style.width = `${normalizedLevel * 100}%`;
      
      // Color coding based on level
      if (normalizedLevel > 0.8) {
        audioLevelBar.className = 'audio-level-bar level-high';
      } else if (normalizedLevel > 0.4) {
        audioLevelBar.className = 'audio-level-bar level-medium';
      } else {
        audioLevelBar.className = 'audio-level-bar level-low';
      }
    }

    function connect() {
      updateConnectionStatus('connecting');
      const baseUrl = wsUrlSelectEl.value;
      const selectedLanguage = languageEl.value;
      
      // Add language parameter to WebSocket URL
      const urlObj = new URL(baseUrl);
      urlObj.searchParams.set('language', selectedLanguage);
      const wsUrl = urlObj.toString();
      
      log(`Connecting with language: ${selectedLanguage}`, 'info');
      
      ws = new WebSocket(wsUrl);
      ws.binaryType = 'arraybuffer';
      
      ws.onopen = () => {
        log('WebSocket connection established', 'success');
        updateConnectionStatus('connected');
        connectionStart = Date.now();
        startCapture();
        btnConnect.disabled = true;
        btnDisconnect.disabled = false;
        btnResetConversation.disabled = false;
      };
      
      ws.onclose = () => {
        log('WebSocket connection closed', 'warning');
        updateConnectionStatus('disconnected');
        stopCapture();
        btnConnect.disabled = false;
        btnDisconnect.disabled = true;
        btnResetConversation.disabled = true;
        connectionStart = null;
      };
      
      ws.onerror = e => {
        log('WebSocket error occurred', 'error');
        updateConnectionStatus('disconnected');
      };
      
      ws.onmessage = e => handleMessage(e.data);
    }

    function disconnect() {
      if (ws) {
        ws.close();
      }
    }

    function resetConversation() {
      if (ws && ws.readyState === WebSocket.OPEN) {
        try {
          const resetMsg = JSON.stringify({
            type: "reset_conversation"
          });
          ws.send(resetMsg);
          log('Sending conversation reset request', 'send');
        } catch (error) {
          log(`Error sending reset: ${error.message}`, 'error');
        }
      } else {
        log('Cannot reset conversation: WebSocket not connected', 'warning');
      }
    }

    function handleMessage(data) {
      try {
        const msg = JSON.parse(data);
        metrics.totalMessages++;
        
        // Extract event type from various message formats
        const eventType = msg.event || msg.type || 'unknown';
        
        // Handle endpoint_event wrapper format
        if (msg.type === 'endpoint_event' && msg.event) {
          handleEndpointEvent(msg);
          return;
        }
        
        // Log message with truncated text for readability
        const textPreview = msg.text ? `- "${msg.text.substring(0, 50)}${msg.text.length > 50 ? '...' : ''}"` : '';
        log(`RX: ${eventType} ${textPreview}`, 'receive');
        
        switch(eventType) {
          case 'partial_transcript':
            handlePartialTranscript(msg);
            break;
            
          case 'final_transcript':
            handleFinalTranscript(msg);
            break;
            
          case 'assistant_text':
          case 'assistant_response':
            handleAssistantResponse(msg);
            break;
            
          case 'assistant_pipeline_start':
            handleAssistantPipelineStart(msg);
            break;
            
          case 'assistant_turn':
            handleAssistantTurn(msg);
            break;
            
          case 'assistant_pipeline_done':
            handleAssistantPipelineDone(msg);
            break;
            
          case 'final_queued':
            handleFinalQueued(msg);
            break;
            
          case 'language_mismatch_notice':
            handleLanguageMismatch(msg);
            break;
            
          case 'metrics_snapshot':
            handleMetricsSnapshot(msg);
            break;
            
          case 'perf_cycle':
            handlePerfCycle(msg);
            break;
            
          case 'heartbeat':
            handleHeartbeat(msg);
            break;
            
          case 'decoder_started':
            log(`ðŸš€ Decoder started for session`, 'info');
            break;
            
          case 'ingest_mode':
            log(`ðŸ“¥ Ingest mode: ${msg.mode || 'unknown'}`, 'info');
            break;
            
          case 'start':
            log(`â–¶ï¸ Utterance start (index: ${msg.utterance_index || 'unknown'})`, 'info');
            break;
            
          case 'final':
            log(`ðŸ Utterance final (index: ${msg.utterance_index || 'unknown'})`, 'success');
            break;
            
          case 'tts_chunk':
          case 'tts_audio':
            metrics.ttsEvents++;
            log(`ðŸ”Š TTS audio received (${msg.encoding || 'unknown'} format)`, 'info');
            break;
            
          case 'tts_start':
            log('ðŸŽµ TTS synthesis started', 'info');
            break;
            
          case 'tts_complete':
            log('âœ… TTS synthesis completed', 'success');
            break;
            
          case 'error':
            log(`âŒ Backend error: ${msg.message} (stage: ${msg.stage || 'unknown'})`, 'error');
            break;
            
          case 'ready':
            log('âœ… Backend ready to receive audio', 'success');
            break;
            
          case 'conversation_reset':
            log('ðŸ”„ Conversation history reset successfully', 'success');
            break;
            
          default:
            // Log unknown message types at debug level instead of warning
            log(`ðŸ”§ Debug event: ${eventType}${textPreview}`, 'debug');
            break;
        }
        
        updateMetrics();
        
      } catch (err) {
        log('Received non-JSON message', 'warning');
      }
    }

    function handleEndpointEvent(msg) {
      const innerEvent = msg.event;
      const utteranceIndex = msg.utterance_index;
      
      log(`ðŸŽ¯ Endpoint: ${innerEvent} (utterance ${utteranceIndex})`, 'info');
      
      // Handle the inner event
      if (innerEvent === 'start') {
        log(`â–¶ï¸ Utterance ${utteranceIndex} started`, 'info');
      } else if (innerEvent === 'final') {
        log(`ðŸ Utterance ${utteranceIndex} finalized`, 'success');
      }
    }

    function handlePartialTranscript(msg) {
      currentPartial = msg.text;
      metrics.transcriptionEvents++;
      
      // Update transcription view
      const lines = transcriptEl.value.split('\n').filter(l => !l.startsWith('[PARTIAL]'));
      transcriptEl.value = lines.join('\n');
      if (currentPartial) {
        transcriptEl.value += `\n[PARTIAL] ${currentPartial}`;
      }
    }

    function handleFinalTranscript(msg) {
      const finalText = msg.text;
      transcriptEl.value += `\n${finalText}`;
      addConversationMessage('user', finalText);
      metrics.transcriptionEvents++;
      currentPartial = '';
      
      log(`ðŸŽ¯ Final transcript received - expecting assistant response...`, 'info');
    }

    function handleAssistantResponse(msg) {
      const utteranceIndex = msg.utterance_index;
      const assistantText = msg.text;
      
      // Deduplicate by utterance_index
      if (utteranceIndex !== undefined) {
        if (assistantResponses.has(utteranceIndex)) {
          log(`ðŸ”„ Duplicate assistant response for utterance ${utteranceIndex} - ignoring`, 'debug');
          return;
        }
        assistantResponses.set(utteranceIndex, { text: assistantText, timestamp: Date.now() });
      }
      
      assistantEl.value += `\n${assistantText}`;
      addConversationMessage('assistant', assistantText);
      
      log(`âœ… Assistant response received for utterance ${utteranceIndex}!`, 'success');
    }

    function handleAssistantPipelineStart(msg) {
      const utteranceIndex = msg.utterance_index || 'unknown';
      log(`ðŸš€ Assistant pipeline started for utterance ${utteranceIndex}`, 'info');
    }

    function handleAssistantTurn(msg) {
      const utteranceIndex = msg.utterance_index || 'unknown';
      log(`ðŸ”„ Assistant turn completed for utterance ${utteranceIndex}`, 'success');
    }

    function handleAssistantPipelineDone(msg) {
      const utteranceIndex = msg.utterance_index || 'unknown';
      log(`âœ… Assistant pipeline done for utterance ${utteranceIndex}`, 'success');
    }

    function handleFinalQueued(msg) {
      const utteranceIndex = msg.utterance_index || 'unknown';
      log(`ðŸ“¥ Final transcript queued for utterance ${utteranceIndex}`, 'info');
    }

    function handleLanguageMismatch(msg) {
      const detected = msg.detected_language || msg.detected || 'unknown';
      const expected = msg.expected_language || msg.expected || document.getElementById('language').value;
      
      log(`ðŸŒ Language mismatch: detected ${detected}, expected ${expected}`, 'warning');
      showLanguageMismatchBanner(detected, expected);
    }

    function handleMetricsSnapshot(msg) {
      log(`ðŸ“Š Metrics snapshot: ${JSON.stringify(msg.metrics || msg)}`, 'debug');
    }

    function handlePerfCycle(msg) {
      const decodeMs = msg.decode_ms || msg.latency_ms || 'unknown';
      const tokens = msg.tokens || 'unknown';
      log(`âš¡ Perf cycle: ${decodeMs}ms, ${tokens} tokens`, 'debug');
    }

    function handleHeartbeat(msg) {
      const bufferUsage = msg.buffer_usage || 'unknown';
      const bytesReceived = msg.bytes_received || 'unknown';
      log(`ðŸ’“ Heartbeat: buffer ${bufferUsage}, bytes ${bytesReceived}`, 'debug');
    }

    async function startCapture() {
      try {
        updateMicStatus('active');
        
        // Create audio context at hardware sample rate for optimal performance
        audioCtx = new (window.AudioContext || window.webkitAudioContext)();
        
        // Get selected devices
        const microphoneDeviceId = document.getElementById('microphoneDevice').value;
        const echoCancellation = document.getElementById('echoCancellation').checked;
        
        // Request microphone access with selected device and settings
        const constraints = { 
          audio: { 
            channelCount: 1,
            echoCancellation: echoCancellation,
            noiseSuppression: true,
            autoGainControl: false // Disable AGC for consistent levels
          } 
        };
        
        // Use specific device if selected
        if (microphoneDeviceId) {
          constraints.audio.deviceId = { exact: microphoneDeviceId };
        }
        
        mediaStream = await navigator.mediaDevices.getUserMedia(constraints);
        
        source = audioCtx.createMediaStreamSource(mediaStream);
        
        // Load AudioWorklet processor for low-latency processing
        await audioCtx.audioWorklet.addModule('/static/js/audio-processor-worklet.js');
        
        // Create AudioWorklet node with configuration
        const chunkSizeMs = parseInt(chunkMsEl.value, 10) || 200; // Default 200ms to prevent word splitting
        processor = new AudioWorkletNode(audioCtx, 'audio-processor', {
          processorOptions: {
            targetSampleRate: 16000,
            chunkSizeMs: chunkSizeMs
          }
        });
        
        // Handle messages from AudioWorklet
        processor.port.onmessage = (event) => {
          const { type, data, level, sampleRate, samples } = event.data;
          
          if (type === 'audioLevel') {
            // Update audio level visualization and tracking
            currentAudioLevel = level;
            updateAudioLevel(level);
            
            // Track audio level history for smarter thresholding
            audioLevelHistory.push(level);
            if (audioLevelHistory.length > 100) { // Keep last 100 samples (about 2 seconds)
              audioLevelHistory.shift();
            }
            
          } else if (type === 'audioChunk') {
            // Send audio chunk to WebSocket with intelligent thresholding
            // This prevents sending silent/garbage audio that confuses Whisper
            const minAudioThreshold = 0.0001; // Base minimum RMS level (lowered from 0.001 for better speech detection)
            const now = Date.now();
            
            // Calculate dynamic threshold based on recent audio levels
            const avgLevel = audioLevelHistory.length > 0 ? 
              audioLevelHistory.reduce((a, b) => a + b, 0) / audioLevelHistory.length : 0;
            const dynamicThreshold = Math.max(minAudioThreshold, avgLevel * 0.1);
            
            if (ws && ws.readyState === WebSocket.OPEN) {
              if (currentAudioLevel > dynamicThreshold) {
                ws.send(data.buffer);
                metrics.audioChunksSent++;
                lastSignificantAudio = now;
                
                // Calculate and display data rate
                const chunkSizeMs = parseInt(chunkMsEl.value, 10) || 200;
                const dataRate = (data.buffer.byteLength * 1000 / chunkSizeMs / 1024).toFixed(1);
                document.getElementById('data-rate').textContent = `${dataRate} KB/s`;
              } else {
                // Log when we're filtering out low-level audio (throttled)
                if (metrics.audioChunksSent % 200 === 0 && now - lastSignificantAudio > 1000) { 
                  log(`ðŸ”‡ Filtering quiet audio (RMS: ${currentAudioLevel.toFixed(6)}, threshold: ${dynamicThreshold.toFixed(6)})`, 'debug');
                }
              }
            }
          }
        };
        
        // Connect audio pipeline
        source.connect(processor);
        
        log(`Audio capture started (${audioCtx.sampleRate}Hz â†’ 16kHz, ${chunkSizeMs}ms chunks)`, 'success');
        
      } catch (error) {
        log(`Failed to start audio capture: ${error.message}`, 'error');
        updateMicStatus('inactive');
        
        // Fallback to ScriptProcessor if AudioWorklet not supported
        if (error.message.includes('AudioWorklet') || error.message.includes('addModule')) {
          log('AudioWorklet not supported, falling back to ScriptProcessor', 'warning');
          await startCaptureFallback();
        }
      }
    }

    async function startCaptureFallback() {
      try {
        // Fallback to ScriptProcessor for older browsers
        audioCtx = new (window.AudioContext || window.webkitAudioContext)({ sampleRate: 16000 });
        
        analyser = audioCtx.createAnalyser();
        analyser.fftSize = 2048;
        
        processor = audioCtx.createScriptProcessor(4096, 1, 1);
        
        const chunkSizeMs = parseInt(chunkMsEl.value, 10) || 200; // Default to 200ms chunks instead of 20ms
        const framesPerChunk = Math.round(audioCtx.sampleRate * (chunkSizeMs / 1000));
        let acc = new Float32Array(0);
        
        processor.onaudioprocess = e => {
          const input = e.inputBuffer.getChannelData(0);
          
          // Calculate audio level for visualization
          let sum = 0;
          for (let i = 0; i < input.length; i++) {
            sum += input[i] * input[i];
          }
          const rms = Math.sqrt(sum / input.length);
          updateAudioLevel(rms);
          
          // Accumulate audio for sending
          const merged = new Float32Array(acc.length + input.length);
          merged.set(acc, 0);
          merged.set(input, acc.length);
          acc = merged;
          
          if (acc.length >= framesPerChunk) {
            const slice = acc.slice(0, framesPerChunk);
            acc = acc.slice(framesPerChunk);
            
            if (ws && ws.readyState === WebSocket.OPEN) {
              const int16 = AudioUtils.floatToInt16(slice);
              ws.send(int16.buffer);
              metrics.audioChunksSent++;
              
              // Update data rate
              const dataRate = (int16.buffer.byteLength * 1000 / chunkSizeMs / 1024).toFixed(1);
              document.getElementById('data-rate').textContent = `${dataRate} KB/s`;
            }
          }
        };
        
        source.connect(analyser);
        source.connect(processor);
        processor.connect(audioCtx.destination);
        
        log('Audio capture started (ScriptProcessor fallback)', 'success');
        
      } catch (error) {
        log(`Fallback audio capture failed: ${error.message}`, 'error');
        updateMicStatus('inactive');
      }
    }

    function stopCapture() {
      updateMicStatus('inactive');
      
      if (processor) {
        if (processor.port) {
          // AudioWorklet cleanup
          processor.port.onmessage = null;
        } else {
          // ScriptProcessor cleanup
          processor.onaudioprocess = null;
        }
        processor.disconnect();
      }
      if (source) {
        source.disconnect();
      }
      if (analyser) {
        analyser.disconnect();
      }
      if (mediaStream) {
        mediaStream.getTracks().forEach(t => t.stop());
      }
      if (audioCtx) {
        audioCtx.close();
      }
      
      // Reset audio level bar
      audioLevelBar.style.width = '0%';
      document.getElementById('data-rate').textContent = '-- KB/s';
      
      log('Audio capture stopped', 'info');
    }

    function clearAll() {
      logEl.textContent = '';
      transcriptEl.value = '';
      assistantEl.value = '';
      conversationContainer.innerHTML = '';
      
      // Reset metrics
      metrics = {
        totalMessages: 0,
        audioChunksSent: 0,
        transcriptionEvents: 0,
        ttsEvents: 0,
        responseTimes: [],
        lastActivity: null
      };
      
      logEntries = [];
      conversationTurns = [];
      currentUtteranceStart = null;
      
      // Reset assistant deduplication tracking
      assistantResponses.clear();
      lastLanguageMismatch = null;
      hideLanguageMismatchBanner();
      
      document.getElementById('turn-count').textContent = '0';
      document.getElementById('avg-response-time').textContent = '-- ms';
      document.getElementById('latency-display').textContent = '-- ms';
      
      updateMetrics();
      log('All data cleared', 'info');
    }

    function exportLogs() {
      const exportData = {
        timestamp: new Date().toISOString(),
        connectionInfo: {
          url: wsUrlSelectEl.value,
          language: languageEl.value,
          chunkMs: chunkMsEl.value
        },
        metrics: metrics,
        logEntries: logEntries,
        conversationTurns: conversationTurns,
        transcription: transcriptEl.value,
        assistantResponses: assistantEl.value
      };
      
      const blob = new Blob([JSON.stringify(exportData, null, 2)], { 
        type: 'application/json' 
      });
      
      const url = URL.createObjectURL(blob);
      const a = document.createElement('a');
      a.href = url;
      a.download = `voice-streaming-debug-${new Date().toISOString().slice(0, 19).replace(/:/g, '-')}.json`;
      document.body.appendChild(a);
      a.click();
      document.body.removeChild(a);
      URL.revokeObjectURL(url);
      
      log('Debug data exported successfully', 'success');
    }

    // Update metrics every second
    setInterval(updateMetrics, 1000);

    // Audio Device Management
    let currentDevices = { audioInput: [], audioOutput: [] };
    
    async function refreshAudioDevices() {
      try {
        const devices = await navigator.mediaDevices.enumerateDevices();
        currentDevices.audioInput = devices.filter(d => d.kind === 'audioinput');
        currentDevices.audioOutput = devices.filter(d => d.kind === 'audiooutput');
        
        // Update microphone dropdown
        const micSelect = document.getElementById('microphoneDevice');
        micSelect.innerHTML = '<option value="">Default</option>';
        currentDevices.audioInput.forEach(device => {
          const option = document.createElement('option');
          option.value = device.deviceId;
          option.textContent = device.label || `Microphone ${device.deviceId.slice(0, 8)}`;
          micSelect.appendChild(option);
        });
        
        // Update speaker dropdown
        const speakerSelect = document.getElementById('speakerDevice');
        speakerSelect.innerHTML = '<option value="">Default</option>';
        currentDevices.audioOutput.forEach(device => {
          const option = document.createElement('option');
          option.value = device.deviceId;
          option.textContent = device.label || `Speaker ${device.deviceId.slice(0, 8)}`;
          speakerSelect.appendChild(option);
        });
        
        log(`Found ${currentDevices.audioInput.length} microphones, ${currentDevices.audioOutput.length} speakers`, 'info');
        
      } catch (error) {
        log(`Device enumeration failed: ${error.message}`, 'error');
      }
    }
    
    // Echo Test Functionality
    let echoTestActive = false;
    let echoTestAudio = null;
    let echoTestStartTime = null;
    
    function startEchoTest() {
      if (echoTestActive) return;
      
      echoTestActive = true;
      echoTestStartTime = Date.now();
      
      // Show results panel
      const resultsPanel = document.getElementById('echo-test-results');
      resultsPanel.style.display = 'block';
      
      // Update UI
      document.getElementById('btn-echo-test').disabled = true;
      document.getElementById('btn-stop-echo-test').disabled = false;
      
      // Play a test tone through TTS system
      echoTestAudio = new Audio('data:audio/wav;base64,UklGRnoGAABXQVZFZm10IBAAAAABAAEAQB8AAEAfAAABAAgAZGF0YQoGAACBhYqFbF1fdJivrJBhNjVgodDbq2EcBj+a2/LDciUFLIHO8tiJNwgZaLvt559NEAxQp+PwtmMcBjiR1/LMeSwFJHfH8N2QQAoUYrTp66hVFApGn+D4rn0+CCSFrfPhqZISAFWl4v6jUg8dXqzl7p1SEA');
      
      echoTestAudio.onended = () => {
        if (echoTestActive) {
          // Continue test until stopped
          echoTestAudio.currentTime = 0;
          echoTestAudio.play().catch(e => log(`Echo test playback failed: ${e.message}`, 'error'));
        }
      };
      
      echoTestAudio.play().catch(e => log(`Echo test failed to start: ${e.message}`, 'error'));
      
      log('Echo test started - listening for echo/feedback', 'info');
      
      // Simulate echo metrics updates (in real implementation, these would come from backend)
      updateEchoMetrics();
    }
    
    function stopEchoTest() {
      if (!echoTestActive) return;
      
      echoTestActive = false;
      
      // Stop audio
      if (echoTestAudio) {
        echoTestAudio.pause();
        echoTestAudio = null;
      }
      
      // Update UI
      document.getElementById('btn-echo-test').disabled = false;
      document.getElementById('btn-stop-echo-test').disabled = true;
      
      // Hide results after a delay
      setTimeout(() => {
        const resultsPanel = document.getElementById('echo-test-results');
        resultsPanel.style.display = 'none';
      }, 3000);
      
      const duration = ((Date.now() - echoTestStartTime) / 1000).toFixed(1);
      log(`Echo test completed (${duration}s)`, 'info');
    }
    
    function updateEchoMetrics() {
      if (!echoTestActive) return;
      
      // In a real implementation, these metrics would come from the echo suppression service
      // For now, simulate realistic values
      const correlation = Math.random() * 0.3; // Low correlation indicates good echo suppression
      const vadActive = Math.random() > 0.7; // Random VAD activity
      const echoDetected = correlation > 0.15; // Threshold for echo detection
      
      // Update display
      const echoDetectedEl = document.getElementById('echo-detected');
      const correlationEl = document.getElementById('echo-correlation');
      const vadActivityEl = document.getElementById('vad-activity');
      
      echoDetectedEl.textContent = echoDetected ? 'Yes' : 'No';
      echoDetectedEl.className = `metric-value ${echoDetected ? 'positive' : 'negative'}`;
      
      correlationEl.textContent = correlation.toFixed(3);
      
      vadActivityEl.textContent = vadActive ? 'Active' : 'Inactive';
      vadActivityEl.className = `metric-value ${vadActive ? 'active' : ''}`;
      
      // Continue updating if test is active
      if (echoTestActive) {
        setTimeout(updateEchoMetrics, 500);
      }
    }

    // Event listeners
    btnConnect.onclick = connect;
    btnDisconnect.onclick = disconnect;
    btnResetConversation.onclick = resetConversation;
    btnClear.onclick = clearAll;
    btnExport.onclick = exportLogs;
    
    // Device management listeners
    document.getElementById('btn-refresh-devices').onclick = refreshAudioDevices;

    // Performance Dashboard Controls
    document.getElementById('btn-refresh-dashboard').onclick = function() {
      const frame = document.getElementById('performance-dashboard-frame');
      const currentSrc = frame.src;
      frame.src = currentSrc + '&t=' + Date.now(); // Force refresh
      log('Performance dashboard refreshed', 'info');
    };
    
    document.getElementById('btn-toggle-dashboard').onclick = function() {
      const dashboard = document.querySelector('.performance-dashboard');
      const btn = document.getElementById('btn-toggle-dashboard');
      const icon = btn.querySelector('i');
      
      if (dashboard.style.display === 'none') {
        dashboard.style.display = 'block';
        icon.className = 'fas fa-eye';
        btn.innerHTML = '<i class="fas fa-eye"></i> Hide';
      } else {
        dashboard.style.display = 'none';
        icon.className = 'fas fa-eye-slash';
        btn.innerHTML = '<i class="fas fa-eye-slash"></i> Show';
      }
      log(`Performance dashboard ${dashboard.style.display === 'none' ? 'hidden' : 'shown'}`, 'info');
    };
    
    document.getElementById('dashboard-metrics').onchange = function(e) {
      const frame = document.getElementById('performance-dashboard-frame');
      const metrics = e.target.value;
      const baseUrl = '/api/v1/performance/dashboard?embed=true';
      frame.src = `${baseUrl}&metrics=${metrics}&t=${Date.now()}`;
      log(`Dashboard metrics changed to: ${metrics}`, 'info');
    };

    // Device selection listeners
    document.getElementById('microphoneDevice').onchange = function(e) {
      if (window.voiceClient) {
        window.voiceClient.selectedMicDevice = e.target.value || null;
        if (window.voiceClient._connected) {
          window.voiceClient.stopCapture && window.voiceClient.stopCapture();
          window.voiceClient.initAudio && window.voiceClient.initAudio();
        }
        log(`Microphone device changed: ${e.target.value}`, 'info');
      }
    };
    document.getElementById('speakerDevice').onchange = function(e) {
      if (window.voiceClient && window.voiceClient.ttsPlayer) {
        window.voiceClient.selectedSpeakerDevice = e.target.value || null;
        window.voiceClient.ttsPlayer.setOutputDevice(e.target.value || null);
        log(`Speaker device changed: ${e.target.value}`, 'info');
      }
    };
    document.getElementById('echoCancellation').onchange = function(e) {
      if (window.voiceClient) {
        window.voiceClient.echoCancellation = e.target.checked;
        log(`Echo cancellation ${e.target.checked ? 'enabled' : 'disabled'}`, 'info');
      }
    };
    document.getElementById('duplexMode').onchange = function(e) {
      if (window.voiceClient) {
        window.voiceClient.duplexEnabled = e.target.checked;
        log(`Duplex mode ${e.target.checked ? 'enabled' : 'disabled'}`, 'info');
      }
    };

    // Echo test listeners
    document.getElementById('btn-echo-test').onclick = startEchoTest;
    document.getElementById('btn-stop-echo-test').onclick = stopEchoTest;

    // Language mismatch banner event listeners
    document.getElementById('btn-switch-language').onclick = switchLanguage;
    document.getElementById('btn-ignore-mismatch').onclick = hideLanguageMismatchBanner;
    document.getElementById('btn-close-banner').onclick = hideLanguageMismatchBanner;

    // Initialize
    updateConnectionStatus('disconnected');
    updateMicStatus('inactive');
    
    // Initialize device list
    refreshAudioDevices();
    
    log('Debug console initialized', 'info');
  </script>
  <script src="{{ url_for('static', filename='js/audioUtils.js') }}"></script>
</body>
</html>
