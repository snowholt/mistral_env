{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a562389c",
   "metadata": {},
   "source": [
    "# Chat Debug Notebook (Ultra Simple)\n",
    "\n",
    "Pure minimal loop calling `ChatService.chat()` with **no history** and **single user role only**.\n",
    "Each round is an independent call; the model does not see prior rounds.\n",
    "\n",
    "## Usage\n",
    "1. Run the environment cell.\n",
    "2. Adjust parameters in the configuration cell.\n",
    "3. Run the execute cell to print N independent answers.\n",
    "\n",
    "Optional: set `CHAT_STRICT_LANGUAGE_ENFORCEMENT=0` (see optional cell) before running to disable Arabic enforcement.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7227f44e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-17 05:18:35,390 | INFO | beautyai_inference.services.inference.content_filter_service | Content filter initialized - Allowed: 252 medical keywords, Forbidden: 103 inappropriate keywords\n",
      "2025-08-17 05:18:35,391 | INFO | beautyai_inference.services.inference.chat_service | Loading fastest model for 24/7 service: qwen3-unsloth-q4ks (unsloth/Qwen3-14B-GGUF)\n",
      "2025-08-17 05:18:35,391 | INFO | beautyai_inference.core.model_manager | Stopped keep-alive timer for model 'qwen3-unsloth-q4ks'\n",
      "2025-08-17 05:18:35,391 | INFO | beautyai_inference.core.model_manager | Started keep-alive timer for model 'qwen3-unsloth-q4ks' (will unload after 60 minutes of inactivity)\n",
      "2025-08-17 05:18:35,392 | INFO | beautyai_inference.services.inference.chat_service | âœ… Fastest model loaded successfully and ready on GPU: qwen3-unsloth-q4ks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using already loaded model 'qwen3-unsloth-q4ks'.\n"
     ]
    }
   ],
   "source": [
    "# Minimal environment / imports (simplified)\n",
    "import os, sys, time\n",
    "from pathlib import Path\n",
    "from typing import List, Dict\n",
    "\n",
    "# Determine backend root heuristically\n",
    "REPO_ROOT = Path.cwd()\n",
    "BACKEND_ROOT = REPO_ROOT / 'backend'\n",
    "if not BACKEND_ROOT.exists():\n",
    "    alt = Path().resolve().parent\n",
    "    if (alt / 'src').exists():\n",
    "        BACKEND_ROOT = alt\n",
    "\n",
    "if str(BACKEND_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(BACKEND_ROOT))\n",
    "SRC_PATH = BACKEND_ROOT / 'src'\n",
    "if SRC_PATH.exists() and str(SRC_PATH) not in sys.path:\n",
    "    sys.path.insert(0, str(SRC_PATH))\n",
    "\n",
    "from beautyai_inference.services.inference.chat_service import ChatService\n",
    "\n",
    "# Create chat service and attempt to load default lightweight model (quietly)\n",
    "chat_service = ChatService()\n",
    "try:\n",
    "    chat_service.load_default_model_from_config()\n",
    "except Exception:\n",
    "    pass  # silently ignore â€“ user may load model elsewhere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "75e31803",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Very simple helper: send the same user message N times, print model reply.\n",
    "# - No accumulated history\n",
    "# - Only user role sent each round\n",
    "# - No logging, no assistant turns stored\n",
    "\n",
    "def run_chat(message: str, language: str, rounds: int = 1):\n",
    "    for i in range(rounds):\n",
    "        print(f\"\\n=== ROUND {i+1} ===\")\n",
    "        result = chat_service.chat(\n",
    "            message=message,\n",
    "            conversation_history=None,  # no history\n",
    "            max_length=max_new_tokens,\n",
    "            language=language,\n",
    "            thinking_mode=False,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "        )\n",
    "        print(f\"USER : {message}\")\n",
    "        print(f\"AI   : {result.get('response')}\")\n",
    "    print(f\"\\nCompleted {rounds} rounds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0e6d6e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "message = '/no_think Ù…Ø§ Ù‡ÙŠ Ø§Ù„Ø¬Ø§Ø°Ø¨ÙŠØ©ØŸ'\n",
    "response_language = 'ar'   # 'auto', 'ar', 'en', 'es', 'fr', 'de'\n",
    "temperature = 0.0\n",
    "top_p = 0.95\n",
    "max_new_tokens = 192\n",
    "rounds = 3  # number of repeated independent calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "12e082e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-17 05:19:43,342 | INFO | beautyai_inference.core.model_manager | Stopped keep-alive timer for model 'qwen3-unsloth-q4ks'\n",
      "2025-08-17 05:19:43,343 | INFO | beautyai_inference.core.model_manager | Started keep-alive timer for model 'qwen3-unsloth-q4ks' (will unload after 60 minutes of inactivity)\n",
      "2025-08-17 05:19:43,343 | INFO | beautyai_inference.services.inference.chat_service | Using persistent default model: qwen3-unsloth-q4ks\n",
      "2025-08-17 05:19:43,343 | INFO | beautyai_inference.services.inference.chat_service | ğŸŒ Using specified language: ar\n",
      "2025-08-17 05:19:43,344 | INFO | beautyai_inference.services.inference.chat_service | Generating response for language: ar\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== ROUND 1 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-17 05:19:44,513 | INFO | beautyai_inference.services.inference.chat_service | [chat][ar_enforce] original_len=243 arabic_chars=197 latin_chars=0 ratio_ar=0.811 min_ratio=0.100 min_ar_chars=8 soft_mode=True\n",
      "2025-08-17 05:19:44,514 | INFO | beautyai_inference.utils.language_detection | Language detection result: ar (confidence: 1.585)\n",
      "2025-08-17 05:19:44,514 | INFO | beautyai_inference.services.inference.chat_service | [chat][ar_enforce] output_language_detected=ar conf=1.585\n",
      "2025-08-17 05:19:44,514 | INFO | beautyai_inference.services.inference.chat_service | Generated response (first 100 chars): Ù¡\n",
      "Ø§Ù„Ø¬Ø§Ø°Ø¨ÙŠØ© Ù‡ÙŠ Ù‚ÙˆØ© Ø·Ø¨ÙŠØ¹ÙŠØ© ØªØ¬Ø°Ø¨ Ø§Ù„Ø£Ø¬Ø³Ø§Ù… Ù†Ø­Ùˆ Ø¨Ø¹Ø¶Ù‡Ø§ Ø§Ù„Ø¨Ø¹Ø¶ØŒ ÙˆØªÙØ¹ØªØ¨Ø± Ù…Ù† Ø§Ù„Ù‚ÙˆÙ‰ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© ÙÙŠ Ø§Ù„ÙƒÙˆÙ†. Ø¹Ù„Ù‰ Ø³Ø·Ø­ Ø§\n",
      "2025-08-17 05:19:44,514 | INFO | beautyai_inference.core.model_manager | Stopped keep-alive timer for model 'qwen3-unsloth-q4ks'\n",
      "2025-08-17 05:19:44,515 | INFO | beautyai_inference.core.model_manager | Started keep-alive timer for model 'qwen3-unsloth-q4ks' (will unload after 60 minutes of inactivity)\n",
      "2025-08-17 05:19:44,515 | INFO | beautyai_inference.services.inference.chat_service | Using persistent default model: qwen3-unsloth-q4ks\n",
      "2025-08-17 05:19:44,515 | INFO | beautyai_inference.services.inference.chat_service | ğŸŒ Using specified language: ar\n",
      "2025-08-17 05:19:44,515 | INFO | beautyai_inference.services.inference.chat_service | Generating response for language: ar\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USER : /no_think Ù…Ø§ Ù‡ÙŠ Ø§Ù„Ø¬Ø§Ø°Ø¨ÙŠØ©ØŸ\n",
      "AI   : Ù¡\n",
      "Ø§Ù„Ø¬Ø§Ø°Ø¨ÙŠØ© Ù‡ÙŠ Ù‚ÙˆØ© Ø·Ø¨ÙŠØ¹ÙŠØ© ØªØ¬Ø°Ø¨ Ø§Ù„Ø£Ø¬Ø³Ø§Ù… Ù†Ø­Ùˆ Ø¨Ø¹Ø¶Ù‡Ø§ Ø§Ù„Ø¨Ø¹Ø¶ØŒ ÙˆØªÙØ¹ØªØ¨Ø± Ù…Ù† Ø§Ù„Ù‚ÙˆÙ‰ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© ÙÙŠ Ø§Ù„ÙƒÙˆÙ†. Ø¹Ù„Ù‰ Ø³Ø·Ø­ Ø§Ù„Ø£Ø±Ø¶ØŒ ØªØ¸Ù‡Ø± Ø§Ù„Ø¬Ø§Ø°Ø¨ÙŠØ© ÙƒÙ‚ÙˆØ© ØªØ¯ÙØ¹ Ø§Ù„Ø£Ø´ÙŠØ§Ø¡ Ù†Ø­Ùˆ Ø§Ù„Ø£Ø³ÙÙ„ØŒ ÙˆÙ‡ÙŠ Ù…Ø§ ÙŠØ³Ù…Ù‰ Ø¨Ø§Ù„ÙˆØ²Ù†. Ù‡Ø°Ù‡ Ø§Ù„Ù‚ÙˆØ© ØªØ¤Ø«Ø± Ø¹Ù„Ù‰ ÙƒÙ„ Ø´ÙŠØ¡ ÙÙŠ Ø§Ù„ÙƒÙˆÙ†ØŒ Ù…Ù† Ø§Ù„ØµØ®ÙˆØ± ÙˆØ§Ù„Ù†Ø¬ÙˆÙ… Ø¥Ù„Ù‰ Ø§Ù„Ø¨Ø´Ø± ÙˆØ§Ù„Ù…Ø¬Ø±Ø§Øª.\n",
      "\n",
      "=== ROUND 2 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-17 05:19:45,652 | INFO | beautyai_inference.services.inference.chat_service | [chat][ar_enforce] original_len=243 arabic_chars=197 latin_chars=0 ratio_ar=0.811 min_ratio=0.100 min_ar_chars=8 soft_mode=True\n",
      "2025-08-17 05:19:45,652 | INFO | beautyai_inference.utils.language_detection | Language detection result: ar (confidence: 1.585)\n",
      "2025-08-17 05:19:45,653 | INFO | beautyai_inference.services.inference.chat_service | [chat][ar_enforce] output_language_detected=ar conf=1.585\n",
      "2025-08-17 05:19:45,653 | INFO | beautyai_inference.services.inference.chat_service | Generated response (first 100 chars): Ù¡\n",
      "Ø§Ù„Ø¬Ø§Ø°Ø¨ÙŠØ© Ù‡ÙŠ Ù‚ÙˆØ© Ø·Ø¨ÙŠØ¹ÙŠØ© ØªØ¬Ø°Ø¨ Ø§Ù„Ø£Ø¬Ø³Ø§Ù… Ù†Ø­Ùˆ Ø¨Ø¹Ø¶Ù‡Ø§ Ø§Ù„Ø¨Ø¹Ø¶ØŒ ÙˆØªÙØ¹ØªØ¨Ø± Ù…Ù† Ø§Ù„Ù‚ÙˆÙ‰ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© ÙÙŠ Ø§Ù„ÙƒÙˆÙ†. Ø¹Ù„Ù‰ Ø³Ø·Ø­ Ø§\n",
      "2025-08-17 05:19:45,653 | INFO | beautyai_inference.core.model_manager | Stopped keep-alive timer for model 'qwen3-unsloth-q4ks'\n",
      "2025-08-17 05:19:45,654 | INFO | beautyai_inference.core.model_manager | Started keep-alive timer for model 'qwen3-unsloth-q4ks' (will unload after 60 minutes of inactivity)\n",
      "2025-08-17 05:19:45,654 | INFO | beautyai_inference.services.inference.chat_service | Using persistent default model: qwen3-unsloth-q4ks\n",
      "2025-08-17 05:19:45,654 | INFO | beautyai_inference.services.inference.chat_service | ğŸŒ Using specified language: ar\n",
      "2025-08-17 05:19:45,654 | INFO | beautyai_inference.services.inference.chat_service | Generating response for language: ar\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USER : /no_think Ù…Ø§ Ù‡ÙŠ Ø§Ù„Ø¬Ø§Ø°Ø¨ÙŠØ©ØŸ\n",
      "AI   : Ù¡\n",
      "Ø§Ù„Ø¬Ø§Ø°Ø¨ÙŠØ© Ù‡ÙŠ Ù‚ÙˆØ© Ø·Ø¨ÙŠØ¹ÙŠØ© ØªØ¬Ø°Ø¨ Ø§Ù„Ø£Ø¬Ø³Ø§Ù… Ù†Ø­Ùˆ Ø¨Ø¹Ø¶Ù‡Ø§ Ø§Ù„Ø¨Ø¹Ø¶ØŒ ÙˆØªÙØ¹ØªØ¨Ø± Ù…Ù† Ø§Ù„Ù‚ÙˆÙ‰ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© ÙÙŠ Ø§Ù„ÙƒÙˆÙ†. Ø¹Ù„Ù‰ Ø³Ø·Ø­ Ø§Ù„Ø£Ø±Ø¶ØŒ ØªØ¸Ù‡Ø± Ø§Ù„Ø¬Ø§Ø°Ø¨ÙŠØ© ÙƒÙ‚ÙˆØ© ØªØ¯ÙØ¹ Ø§Ù„Ø£Ø´ÙŠØ§Ø¡ Ù†Ø­Ùˆ Ø§Ù„Ø£Ø³ÙÙ„ØŒ ÙˆÙ‡ÙŠ Ù…Ø§ ÙŠØ³Ù…Ù‰ Ø¨Ø§Ù„ÙˆØ²Ù†. Ù‡Ø°Ù‡ Ø§Ù„Ù‚ÙˆØ© ØªØ¤Ø«Ø± Ø¹Ù„Ù‰ ÙƒÙ„ Ø´ÙŠØ¡ ÙÙŠ Ø§Ù„ÙƒÙˆÙ†ØŒ Ù…Ù† Ø§Ù„ØµØ®ÙˆØ± ÙˆØ§Ù„Ù†Ø¬ÙˆÙ… Ø¥Ù„Ù‰ Ø§Ù„Ø¨Ø´Ø± ÙˆØ§Ù„Ù…Ø¬Ø±Ø§Øª.\n",
      "\n",
      "=== ROUND 3 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-17 05:19:46,791 | INFO | beautyai_inference.services.inference.chat_service | [chat][ar_enforce] original_len=243 arabic_chars=197 latin_chars=0 ratio_ar=0.811 min_ratio=0.100 min_ar_chars=8 soft_mode=True\n",
      "2025-08-17 05:19:46,792 | INFO | beautyai_inference.utils.language_detection | Language detection result: ar (confidence: 1.585)\n",
      "2025-08-17 05:19:46,792 | INFO | beautyai_inference.services.inference.chat_service | [chat][ar_enforce] output_language_detected=ar conf=1.585\n",
      "2025-08-17 05:19:46,792 | INFO | beautyai_inference.services.inference.chat_service | Generated response (first 100 chars): Ù¡\n",
      "Ø§Ù„Ø¬Ø§Ø°Ø¨ÙŠØ© Ù‡ÙŠ Ù‚ÙˆØ© Ø·Ø¨ÙŠØ¹ÙŠØ© ØªØ¬Ø°Ø¨ Ø§Ù„Ø£Ø¬Ø³Ø§Ù… Ù†Ø­Ùˆ Ø¨Ø¹Ø¶Ù‡Ø§ Ø§Ù„Ø¨Ø¹Ø¶ØŒ ÙˆØªÙØ¹ØªØ¨Ø± Ù…Ù† Ø§Ù„Ù‚ÙˆÙ‰ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© ÙÙŠ Ø§Ù„ÙƒÙˆÙ†. Ø¹Ù„Ù‰ Ø³Ø·Ø­ Ø§\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USER : /no_think Ù…Ø§ Ù‡ÙŠ Ø§Ù„Ø¬Ø§Ø°Ø¨ÙŠØ©ØŸ\n",
      "AI   : Ù¡\n",
      "Ø§Ù„Ø¬Ø§Ø°Ø¨ÙŠØ© Ù‡ÙŠ Ù‚ÙˆØ© Ø·Ø¨ÙŠØ¹ÙŠØ© ØªØ¬Ø°Ø¨ Ø§Ù„Ø£Ø¬Ø³Ø§Ù… Ù†Ø­Ùˆ Ø¨Ø¹Ø¶Ù‡Ø§ Ø§Ù„Ø¨Ø¹Ø¶ØŒ ÙˆØªÙØ¹ØªØ¨Ø± Ù…Ù† Ø§Ù„Ù‚ÙˆÙ‰ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© ÙÙŠ Ø§Ù„ÙƒÙˆÙ†. Ø¹Ù„Ù‰ Ø³Ø·Ø­ Ø§Ù„Ø£Ø±Ø¶ØŒ ØªØ¸Ù‡Ø± Ø§Ù„Ø¬Ø§Ø°Ø¨ÙŠØ© ÙƒÙ‚ÙˆØ© ØªØ¯ÙØ¹ Ø§Ù„Ø£Ø´ÙŠØ§Ø¡ Ù†Ø­Ùˆ Ø§Ù„Ø£Ø³ÙÙ„ØŒ ÙˆÙ‡ÙŠ Ù…Ø§ ÙŠØ³Ù…Ù‰ Ø¨Ø§Ù„ÙˆØ²Ù†. Ù‡Ø°Ù‡ Ø§Ù„Ù‚ÙˆØ© ØªØ¤Ø«Ø± Ø¹Ù„Ù‰ ÙƒÙ„ Ø´ÙŠØ¡ ÙÙŠ Ø§Ù„ÙƒÙˆÙ†ØŒ Ù…Ù† Ø§Ù„ØµØ®ÙˆØ± ÙˆØ§Ù„Ù†Ø¬ÙˆÙ… Ø¥Ù„Ù‰ Ø§Ù„Ø¨Ø´Ø± ÙˆØ§Ù„Ù…Ø¬Ø±Ø§Øª.\n",
      "\n",
      "Completed 3 rounds.\n"
     ]
    }
   ],
   "source": [
    "# Execute simple chat rounds\n",
    "run_chat(message=message, language=response_language, rounds=rounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bcc5d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: disable Arabic enforcement (uncomment if needed)\n",
    "# import os; os.environ['CHAT_STRICT_LANGUAGE_ENFORCEMENT'] = '0'\n",
    "# Then rerun the environment + execute cells."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
