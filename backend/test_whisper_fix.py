#!/usr/bin/env python3
"""Test script to verify the Whisper engine fix"""

import sys
import os
import logging

# Add the beautyai backend to path
sys.path.insert(0, '/home/lumi/beautyai/backend/src')

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(levelname)s:%(name)s:%(message)s')

print("ğŸ§ª Testing Whisper engine fix...")

try:
    from beautyai_inference.core.model_manager import ModelManager
    
    print("âœ… ModelManager imported successfully")
    
    # Create ModelManager instance
    manager = ModelManager()
    print(f"âœ… ModelManager instance created")
    
    # Try to load Whisper model
    print("ğŸ”„ Loading Whisper model...")
    service = manager.get_streaming_whisper('whisper-large-v3-turbo')
    
    if service:
        print("âœ… SUCCESS: Whisper model loaded!")
        
        # Test with a small fake audio array
        import numpy as np
        fake_audio = np.random.random(16000).astype(np.float32)  # 1 second of random audio
        
        print("ğŸ”„ Testing transcription with fake audio...")
        result = service._transcribe_implementation(fake_audio, "en")
        print(f"ğŸ“ Transcription result: '{result}'")
        
        if result and result != "you":
            print("âœ… SUCCESS: Transcription working correctly!")
        else:
            print("âš ï¸ Transcription still returning minimal result, but no crash!")
            
    else:
        print("âŒ FAILED: Could not load Whisper model")
        
except Exception as e:
    print(f"âŒ ERROR: {e}")
    import traceback
    traceback.print_exc()