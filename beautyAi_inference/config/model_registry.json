{
  "default_model": "qwen3-model",
  "models": {
    "qwen3-model": {
      "model_id": "Qwen/Qwen3-14B",
      "engine_type": "transformers",
      "quantization": "4bit",
      "dtype": "bfloat16",
      "max_new_tokens": 1024,
      "name": "qwen3-model",
      "description": "Qwen3 14B model (4-bit quantized)",
      "model_architecture": "causal_lm",
      "documentation": {
        "quantization_notes": "Can use 'bnb_4bit' if using a specific loader",
        "dtype_notes": "bfloat16 is generally preferred if supported, else float16",
        "max_tokens_notes": "Slightly more room for thought process",
        "temperature_notes": "Recommended for thinking mode",
        "top_p_notes": "Recommended for thinking mode", 
        "top_k_notes": "Recommended for thinking mode",
        "presence_penalty_notes": "Qwen specific, helps reduce repetition slightly differently",
        "repetition_penalty_notes": "General repetition penalty",
        "enable_thinking_notes": "For chat template: tokenizer.apply_chat_template(..., enable_thinking=True)"
      },
      "custom_generation_params": {
        "temperature": 0.6,
        "top_p": 0.95,
        "top_k": 20,
        "do_sample": true,
        "presence_penalty": 0.5,
        "repetition_penalty": 1.1,
        "enable_thinking": true
      }
    },
    "bee1reason-arabic-qwen-14b": {
      "model_id": "beetlware/Bee1reason-arabic-Qwen-14B",
      "engine_type": "transformers",
      "quantization": "none",
      "dtype": "float16",
      "max_new_tokens": 1024,
      "name": "bee1reason-arabic-qwen-14b",
      "description": "Arabic-optimized Qwen 14B model (float16)",
      "model_architecture": "causal_lm",
      "documentation": {
        "quantization_notes": "This model is already merged_16bit (float16)",
        "dtype_notes": "As per model card: standalone 16-bit (float16) precision model",
        "temperature_notes": "Aligns with Qwen3 thinking mode & reasoning example",
        "enable_thinking_notes": "Assumed beneficial due to Qwen3 base and reasoning task"
      },
      "custom_generation_params": {
        "temperature": 0.6,
        "top_p": 0.95,
        "top_k": 20,
        "do_sample": true,
        "repetition_penalty": 1.1,
        "enable_thinking": true
      }
    },
    "bee1reason-arabic-qwen-14b-gguf": {
      "model_id": "beetlware/Bee1reason-arabic-Qwen-14B-Q4_K_M-GGUF",
      "engine_type": "llama.cpp",
      "quantization": "Q4_K_M",
      "dtype": "float16",
      "max_new_tokens": 1024,
      "name": "bee1reason-arabic-qwen-14b-gguf",
      "description": "Arabic-optimized Qwen 14B model (GGUF Q4_K_M format)",
      "model_architecture": "causal_lm",
      "documentation": {
        "engine_notes": "More typical for GGUF, adjust if using a transformers wrapper",
        "quantization_notes": "GGUF specific quantization format",
        "dtype_notes": "Less relevant for GGUF, internal GGUF types used",
        "params_notes": "Parameters for llama.cpp might be named differently",
        "enable_thinking_notes": "enable_thinking would be managed by prompt format for GGUF if model supports it"
      },
      "custom_generation_params": {
        "temperature": 0.6,
        "top_p": 0.95,
        "top_k": 20,
        "repetition_penalty": 1.1
      }
    },
    "llama-4-maverick-17b": {
      "model_id": "meta-llama/Llama-4-Maverick-17B-128E-Instruct",
      "engine_type": "transformers",
      "quantization": "4bit",
      "dtype": "bfloat16",
      "max_new_tokens": 1024,
      "name": "llama-4-maverick-17b",
      "description": "Meta Llama-4 Maverick 17B Instruct model",
      "model_architecture": "causal_lm",
      "documentation": {
        "quantization_notes": "Can use 'bnb_4bit' if using a specific loader",
        "dtype_notes": "Llama 4 card mentions bf16 for evals",
        "temperature_notes": "Slightly lower for more controlled instruct responses",
        "system_prompt_suggestion": "Consider adding Llama 4's recommended system prompt: 'You are an expert conversationalist...'"
      },
      "custom_generation_params": {
        "temperature": 0.6,
        "top_p": 0.9,
        "do_sample": true,
        "repetition_penalty": 1.1
      }
    },
    "deepseek-r1-qwen-14b-multilingual": {
      "model_id": "lightblue/DeepSeek-R1-Distill-Qwen-14B-Multilingual",
      "engine_type": "transformers",
      "quantization": "4bit",
      "dtype": "bfloat16",
      "max_new_tokens": 1024,
      "name": "deepseek-r1-qwen-14b-multilingual",
      "description": "DeepSeek R1 Distill Qwen 14B Multilingual model",
      "model_architecture": "causal_lm",
      "documentation": {
        "quantization_notes": "Can use 'bnb_4bit' if using a specific loader",
        "dtype_notes": "bfloat16 or float16 both supported",
        "temperature_notes": "Mid-range of 0.5-0.7 recommendation",
        "repetition_penalty_notes": "As recommended for this model"
      },
      "custom_generation_params": {
        "temperature": 0.6,
        "top_p": 0.95,
        "do_sample": true,
        "repetition_penalty": 1.1
      }
    },
    "arabic-deepseek-r1-distill-8b": {
      "model_id": "Omartificial-Intelligence-Space/Arabic-DeepSeek-R1-Distill-8B",
      "engine_type": "transformers",
      "quantization": "4bit",
      "dtype": "bfloat16",
      "max_new_tokens": 1024,
      "name": "arabic-deepseek-r1-distill-8b",
      "description": "Arabic-optimized DeepSeek R1 Distill 8B model for reasoning",
      "model_architecture": "causal_lm",
      "documentation": {
        "quantization_notes": "Model card says 'load_in_4bit=True'",
        "dtype_notes": "bfloat16 or float16 both supported",
        "max_tokens_notes": "For reasoning steps",
        "temperature_notes": "Lower for math reasoning",
        "repetition_penalty_notes": "Slightly higher due to baseline issues mentioned in card"
      },
      "custom_generation_params": {
        "temperature": 0.5,
        "top_p": 0.9,
        "do_sample": true,
        "repetition_penalty": 1.15
      }
    },
    "deepseek-r1-qwen-14b-multilingual-gguf": {
      "model_id": "pelican7/DeepSeek-R1-Distill-Qwen-14B-Multilingual-Q4_K_M-GGUF",
      "engine_type": "llama.cpp",
      "quantization": "Q4_K_M",
      "dtype": "float16",
      "max_new_tokens": 1024,
      "name": "deepseek-r1-qwen-14b-multilingual-gguf",
      "description": "DeepSeek R1 Distill Qwen 14B Multilingual model (GGUF Q4_K_M format)",
      "model_architecture": "causal_lm",
      "documentation": {
        "engine_notes": "More typical for GGUF format",
        "quantization_notes": "GGUF Q4_K_M specific quantization",
        "dtype_notes": "Less relevant for GGUF, internal GGUF types used",
        "params_notes": "Parameters for llama.cpp might be named differently"
      },
      "custom_generation_params": {
        "temperature": 0.6,
        "top_p": 0.95,
        "repetition_penalty": 1.1
      }
    },

    "arabic-deepseek-r1-distill-llama3-8b": {
      "model_id": "Paula139/DeepSeek-R1-destill-llama3-8b-arabic-fine-tuned",
      "engine_type": "transformers",
      "quantization": "4bit",
      "dtype": "bfloat16",
      "max_new_tokens": 1024,
      "name": "arabic-deepseek-r1-distill-llama3-8b",
      "description": "Arabic fine-tuned DeepSeek R1 Distill Llama3 8B model for reasoning",
      "model_architecture": "causal_lm",
      "documentation": {
        "quantization_notes": "Assuming 4bit, as it's an 8B model and Unsloth is mentioned",
        "dtype_notes": "bfloat16 or float16 both supported",
        "max_tokens_notes": "For reasoning tasks",
        "temperature_notes": "For reasoning tasks"
      },
      "custom_generation_params": {
        "temperature": 0.5,
        "top_p": 0.9,
        "do_sample": true,
        "repetition_penalty": 1.15
      }
    },
    "arabic-morph-deepseek-r1-distill-llama-8b": {
      "model_id": "omarxadel/Arabic-Morph-DeepSeek-R1-Distill-Llama-8B",
      "engine_type": "transformers",
      "quantization": "4bit",
      "dtype": "bfloat16",
      "max_new_tokens": 512,
      "name": "arabic-morph-deepseek-r1-distill-llama-8b",
      "description": "Arabic Morphological DeepSeek R1 Distill Llama 8B model",
      "model_architecture": "causal_lm",
      "documentation": {
        "quantization_notes": "Assuming 4bit, Unsloth mentioned",
        "dtype_notes": "bfloat16 or float16 both supported",
        "max_tokens_notes": "Morphological analysis might not need very long outputs",
        "temperature_notes": "Lower for precise morphological tasks"
      },
      "custom_generation_params": {
        "temperature": 0.3,
        "top_p": 0.9,
        "do_sample": true,
        "repetition_penalty": 1.1
      }
    }
  }
}