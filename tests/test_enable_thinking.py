#!/usr/bin/env python3
"""
Test script to demonstrate enable_thinking parameter functionality.

This script shows how to:
1. Control enable_thinking via model registry custom_generation_params
2. Override enable_thinking via kwargs
3. Test both thinking and non-thinking modes
"""
import sys
import os
sys.path.append('/home/lumi/beautyai')

from beautyai_inference.core.model_manager import ModelManager
from beautyai_inference.config.config_manager import ConfigManager

def test_enable_thinking():
    """Test enable_thinking functionality with different models."""
    
    print("ğŸ§  Testing enable_thinking functionality")
    print("=" * 60)
    
    # Initialize config manager
    config_manager = ConfigManager()
    
    # Test with a model that has enable_thinking=true
    print("\n1ï¸âƒ£ Testing model with enable_thinking=true (qwen3-model)")
    print("-" * 50)
    
    model_config = config_manager.get_model_config("qwen3-model")
    if model_config and model_config.custom_generation_params:
        enable_thinking = model_config.custom_generation_params.get('enable_thinking', 'Not set')
        print(f"âœ… Model custom_generation_params.enable_thinking: {enable_thinking}")
        
        # Show all custom params
        print(f"ğŸ“‹ All custom generation params:")
        for key, value in model_config.custom_generation_params.items():
            print(f"   - {key}: {value}")
    else:
        print("âŒ Model not found or no custom_generation_params")
    
    print("\n2ï¸âƒ£ Testing model with enable_thinking=false (bee1reason-arabic-q4km-i1)")
    print("-" * 50)
    
    model_config = config_manager.get_model_config("bee1reason-arabic-q4km-i1")
    if model_config and model_config.custom_generation_params:
        enable_thinking = model_config.custom_generation_params.get('enable_thinking', 'Not set')
        print(f"âœ… Model custom_generation_params.enable_thinking: {enable_thinking}")
        
        # Show all custom params
        print(f"ğŸ“‹ All custom generation params:")
        for key, value in model_config.custom_generation_params.items():
            print(f"   - {key}: {value}")
    else:
        print("âŒ Model not found or no custom_generation_params")
    
    print("\n3ï¸âƒ£ How enable_thinking flows through the system:")
    print("-" * 50)
    print("ğŸ“Š Parameter Flow:")
    print("   1. Model Registry (model_registry.json)")
    print("      â””â”€â”€ custom_generation_params.enable_thinking")
    print("   2. ModelConfig object")
    print("      â””â”€â”€ config.custom_generation_params['enable_thinking']")
    print("   3. TransformersEngine.chat() method")
    print("      â””â”€â”€ generation_params.get('enable_thinking', False)")
    print("   4. _format_conversation() method")
    print("      â””â”€â”€ tokenizer.apply_chat_template(..., enable_thinking=value)")
    print("   5. Qwen3 model receives thinking instruction")
    print("      â””â”€â”€ Generates <think>...</think> blocks or not")

def test_content_filter_control():
    """Test content filter control via CLI arguments."""
    
    print("\nğŸ›¡ï¸ Content Filter Control Options")
    print("=" * 60)
    
    print("ğŸ“‹ Available Content Filter Levels:")
    print("   - strict: Most restrictive filtering")
    print("   - balanced: Default level (recommended)")
    print("   - relaxed: More permissive")
    print("   - disabled: No content filtering")
    
    print("\nğŸ¯ How to Use:")
    print("   Unfortunately, the --content-filter argument is not yet")
    print("   integrated with the 'run chat' command. Here are alternatives:")
    
    print("\nğŸ“ Option 1: Modify content filter in code")
    print("   from beautyai_inference.services.inference.content_filter_service import ContentFilterService")
    print("   filter_service = ContentFilterService()")
    print("   filter_service.set_strictness_level('disabled')")
    
    print("\nğŸ“ Option 2: Test without content filter")
    print("   The content filter is primarily applied in chat services")
    print("   You can test models directly via the engine interface")

def show_recommendations():
    """Show recommendations for controlling enable_thinking."""
    
    print("\nğŸ’¡ Recommendations for Using enable_thinking")
    print("=" * 60)
    
    print("ğŸ¯ To Control enable_thinking:")
    print("   1. PREFERRED: Set in model registry (model_registry.json)")
    print("      - Persistent across sessions")
    print("      - Model-specific defaults")
    print("      - Example: 'enable_thinking': true/false in custom_generation_params")
    
    print("\n   2. RUNTIME: Override via kwargs in chat() method")
    print("      - Temporary override for specific requests")
    print("      - Example: engine.chat(messages, enable_thinking=False)")
    
    print("\n   3. CLI: Add --enable-thinking / --disable-thinking arguments")
    print("      - Would need CLI argument integration")
    print("      - Currently not implemented")
    
    print("\nğŸ”§ Current Implementation Status:")
    print("   âœ… Model registry integration (WORKING)")
    print("   âœ… TransformersEngine support (WORKING)")
    print("   âœ… Chat template integration (WORKING)")
    print("   âŒ CLI argument support (NEEDS IMPLEMENTATION)")
    print("   âŒ Content filter CLI integration (NEEDS IMPLEMENTATION)")
    
    print("\nğŸ“‹ Next Steps to Test:")
    print("   1. Use model registry settings (already configured)")
    print("   2. Test with simple chat interface")
    print("   3. Compare outputs between enable_thinking=true vs false models")
    print("   4. Verify <think>...</think> blocks appear/disappear correctly")

if __name__ == "__main__":
    test_enable_thinking()
    test_content_filter_control()
    show_recommendations()
    
    print("\nğŸš€ Ready to test! Your enable_thinking setup is complete.")
    print("   The models in your registry are properly configured.")
    print("   The TransformersEngine will use the enable_thinking parameter.")
    print("   Try running a simple chat to see the difference!")
